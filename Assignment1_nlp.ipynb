{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">344.105/6/7 UE: Natural Language Processing (WS2022/23)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 1</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Document Classification with Standard Machine Learning Methods</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University (JKU) Linz, and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Authors:** Navid Rekab-saz, Oleg Lesota<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-preprocessing\"><li style=\"font-size:large;font-weight:bold\">Task A: Pre-processing & Feature Extraction (15 points)</li></a>\n",
    "    <a href=\"#section-training\"><li style=\"font-size:large;font-weight:bold\">Task B: Training and Results Analysis (15 points)</li></a>\n",
    "    <a href=\"#section-optional\"><li style=\"font-size:large;font-weight:bold\">Task C: Linear Model Interpretability (2 extra point)</li></a>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "### Assignment objective\n",
    "\n",
    "The aim of this assignment is to implement a document (sentence) classification model using (standard) machine learning methods. The assignment in total has **30 points**; it also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "### Libraries & Dataset\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python` (>3.7). Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "To conduct the experiments, we use a subset of the `HumSet` dataset [1] (https://blog.thedeep.io/humset/). `HumSet` is created by the DEEP (https://www.thedeep.io) project – an open source platform which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset contains the classes (labels) referring to the humanitarian sectors like agriculture, health, and protection. The dataset contains an overall number of 17,301 data points. \n",
    "\n",
    "Download the dataset from the Moodle page of the course.\n",
    "\n",
    "the provided zip file consists of the following files:\n",
    "- `thedeep.subset.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.labels.txt`: Captions of the labels.\n",
    "- `thedeep.ToU.txt`: Terms of use of the dataset.\n",
    "\n",
    "[1] HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response\n",
    "*Selim Fekih, Nicolo' Tamagnone, Benjamin Minixhofer, Ranjan Shrestha, Ximena Contla, Ewan Oglethorpe and Navid Rekabsaz.* \n",
    "In Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP), December 2022.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "### Submission\n",
    "\n",
    "Each group should submit the following two files:\n",
    "\n",
    "- One Jupyter Notebook file (`.ipynb`), containing all the code, results, visualizations, etc. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that (if necessary) one can run all the cells from top to bottom without any error. Do not forget to put in your names and student numbers in the first cell of the Notebook. \n",
    "- The HTML file (`.html`) achieved from exporting the Jupyter Notebook to HTML (Download As HTML).\n",
    "\n",
    "You do not need to include the data files in the submission.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-preprocessing\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Pre-processing & Feature Extraction (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "    \n",
    "**Preprocessing (5 points).** Load the train, validation, and test sets. Study the text and according to your judgements, apply at least <ins>two text cleaning/preprocessing methods</ins>. Punctuations marks, numbers, dates, case-sensitivity are some examples of the elements which can be potentially considered for cleaning/preprocessing. Tokenize the result text with a tokenizer of your choice. Report your approaches to text cleaning and tokenization and the reasons of your choices. Provide some examples, showing the effects of the applied approaches on the text.\n",
    "\n",
    "**Creating dictionary (5 points).** Create a dictionary of vocabularies following the guidelines discussed in the lecture. Next, reduce the size of dictionary using a method of your choice, for instance by considering a cut-off threshold on the tokens with low frequencies. When removing tokens from the dictionary, consider a strategy for handling Out-Of-Vocabulary (OOV) tokens, namely the ones in the train/validation/test datasets that that are not anymore in the dictionary. Some possible strategies could be to remove OOVs completely from the texts, or to replace them with a special token like <OOV\\>. Explain your approaches and report the statistics of the dictionary before and after the reduction.\n",
    "\n",
    "**Creating sentence vectors (5 points).** Use the dictionary to prepare <ins>two variations of document representation vectors</ins>, separately for train, validation, and test sets. Both variations follow a Bag-of-Words approach with a different token weighting method. One applied weighting must be `tf-idf` and the other one can be any other method discussed in the lecture such as `tc`, `tf`, `BM25`. These term weighting methods should be implemented; using a library to readily calculate the term weightings is not allowed. Report the applied approaches. Calculate and report the sparsity rate of the vectors of train, validation, and test sets, namely what percentages of the vectors in each set are filled with zeros.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\irina\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define data pathes\n",
    "\n",
    "\n",
    "train_data_path = \"C:/Users/irina/Documents/AI/NLP/nlp2023_24_data/thedeep.subset.train.txt\"\n",
    "test_data_path = \"C:/Users/irina/Documents/AI/NLP/nlp2023_24_data/thedeep.subset.test.txt\"\n",
    "val_data_path = \"C:/Users/irina/Documents/AI/NLP/nlp2023_24_data/thedeep.subset.validation.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import Bunch\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(input_file: str):\n",
    "    \"\"\"\n",
    "    A function for loading a dataset from a txt file\n",
    "    \"\"\"\n",
    "    #bunch = Bunch() #sklearn Bunch is a simple holder object with fields that can be both accessed as python dict keys or object attributes\n",
    "\n",
    "    docs = {}\n",
    "    tokens = {}\n",
    "    dictionary = set()\n",
    "    count = {}\n",
    "    # Regular expression pattern to match the format and extract information:\n",
    "    pattern = re.compile(r'(\\d*),((.)+),(\\d)')\n",
    "    with open(input_file, 'r', encoding = 'utf-8') as f:\n",
    "        try:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                entry = line.strip()\n",
    "                match = re.match(pattern, entry)\n",
    "                if match:\n",
    "                    # Extract values from the matched groups\n",
    "                    sentence_id = int(match.group(1))\n",
    "                    text = match.group(2)\n",
    "                    text = text.translate(str.maketrans('', '', punctuation))\n",
    "                    text = text.lower()\n",
    "                    label = int(match.group(4))\n",
    "                    #bunch[sentence_id] = {\"text\": text, \"label\": label}\n",
    "                    docs[sentence_id] = {\"text\": text, \"label\": label}\n",
    "                    tokens[sentence_id] = {\"tokenized\": nltk.word_tokenize(text), \"label\": label}\n",
    "                    for token in tokens[sentence_id][\"tokenized\"]:\n",
    "                        if not re.match(r'-?\\d+', token): # do not add numbers and other nummerical characters to the dictionary\n",
    "                            if token not in dictionary:\n",
    "                                dictionary.add(token)\n",
    "                                count[token] = 1\n",
    "                            else:\n",
    "                                count[token] +=1 \n",
    "                else:\n",
    "                    print(f\"no match for {line=}\")\n",
    "                line = f.readline()\n",
    "        except UnicodeDecodeError as err:\n",
    "            print(err)\n",
    "\n",
    "\n",
    "    #remove too infrequent words\n",
    "    # not sure if that's a good idea (since actually the infrequent words carry more information?!), so commented out for now. for the record: if all the words that occur less then 10 times are removed, the size of \n",
    "    #the dictionary is halfed (6216 tokens)\n",
    "    #for word, ct in count.items():\n",
    "     #   if ct < 10:\n",
    "      #      dictionary.discard(word)\n",
    "\n",
    "    \n",
    "        \n",
    "    \n",
    "    return tokens, dictionary, count, docs #,bunch, dic, \n",
    "\n",
    "\n",
    "train_tokens, train_dictionary, train_wordcount, train_docs= load_dataset(train_data_path)\n",
    "#train_bunch, train_dic we probably won't need, otherwise uncomment\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_docs[9655]={'text': 'a food crisis is looming in the country with this season’s maize harvest expected to be 20 per cent less than the projected 40 million bags due to erratic rainfall coupled with fall armyworm invasion in the grain baskets the drought experienced in parts of north rift mid this year resulted in more than 40000 acres of maize crop in uasin gishu county alone withering this translates to a total loss of about 800000 bags of maize normally an acre produces at least 20 bags uasin gishu county executive in charge of agriculture dr cyril cheruiyot said most of the affected crops were those in tasselling stages of growth “the last time a drought of that magnitude was experienced in this region was in 1984 at least we have started receiving rain meaning the entire crop is not affected” he said', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "#test printout of train_docs (works exactly the same as train_bunch)\n",
    "\n",
    "#bunch probably not needed?\n",
    "print(f\"{train_docs[9655]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_tokens[9655]={'tokenized': ['a', 'food', 'crisis', 'is', 'looming', 'in', 'the', 'country', 'with', 'this', 'season', '’', 's', 'maize', 'harvest', 'expected', 'to', 'be', '20', 'per', 'cent', 'less', 'than', 'the', 'projected', '40', 'million', 'bags', 'due', 'to', 'erratic', 'rainfall', 'coupled', 'with', 'fall', 'armyworm', 'invasion', 'in', 'the', 'grain', 'baskets', 'the', 'drought', 'experienced', 'in', 'parts', 'of', 'north', 'rift', 'mid', 'this', 'year', 'resulted', 'in', 'more', 'than', '40000', 'acres', 'of', 'maize', 'crop', 'in', 'uasin', 'gishu', 'county', 'alone', 'withering', 'this', 'translates', 'to', 'a', 'total', 'loss', 'of', 'about', '800000', 'bags', 'of', 'maize', 'normally', 'an', 'acre', 'produces', 'at', 'least', '20', 'bags', 'uasin', 'gishu', 'county', 'executive', 'in', 'charge', 'of', 'agriculture', 'dr', 'cyril', 'cheruiyot', 'said', 'most', 'of', 'the', 'affected', 'crops', 'were', 'those', 'in', 'tasselling', 'stages', 'of', 'growth', '“', 'the', 'last', 'time', 'a', 'drought', 'of', 'that', 'magnitude', 'was', 'experienced', 'in', 'this', 'region', 'was', 'in', '1984', 'at', 'least', 'we', 'have', 'started', 'receiving', 'rain', 'meaning', 'the', 'entire', 'crop', 'is', 'not', 'affected', '”', 'he', 'said'], 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "#test tokenized text with nltk. seems to work\n",
    "print(f\"{train_tokens[9655]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30875"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how many words do we have altogether in the dictionary\n",
    "len(train_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leishmaniosis',\n",
       " 'twentythree',\n",
       " 'pumping',\n",
       " 'mathow',\n",
       " 'spe',\n",
       " 'mughayyir',\n",
       " 'napalmlike',\n",
       " 'mentao',\n",
       " 'ein',\n",
       " 'swedisheritrean',\n",
       " 'copán',\n",
       " 'oppression',\n",
       " 'hurricaneweakened',\n",
       " 'women53',\n",
       " 'rumek',\n",
       " 'denguerelated',\n",
       " 'radio',\n",
       " 'resumption',\n",
       " 'us85',\n",
       " 'cohort',\n",
       " 'pulse',\n",
       " 'enhanced',\n",
       " 'akech',\n",
       " 'same',\n",
       " 'closeknit',\n",
       " 'reintegrating',\n",
       " 'com\\xadm\\xadissio\\xadner',\n",
       " 'plummeting',\n",
       " 'churoamaya',\n",
       " 'mobilise',\n",
       " 'literally',\n",
       " 'centreouest',\n",
       " 'schoolthe',\n",
       " 'tracts',\n",
       " 'just',\n",
       " 'hotbed',\n",
       " 'laughing',\n",
       " 'shomolu',\n",
       " 'mogadisho',\n",
       " 'sanabel',\n",
       " 'flurry',\n",
       " 'kaskazini',\n",
       " 'vieja',\n",
       " 'warplanes',\n",
       " 'sheikhoun',\n",
       " 'supportwhile',\n",
       " 'obeid',\n",
       " 'baracoa',\n",
       " 'prang',\n",
       " 'beheloka',\n",
       " 'administering',\n",
       " 'osun',\n",
       " 'lokshina',\n",
       " 'unprecedented',\n",
       " 'atatrah',\n",
       " 'suriname',\n",
       " 'manofoli',\n",
       " 'splintered',\n",
       " 'sittwe',\n",
       " 'is',\n",
       " 'andunclean',\n",
       " 'bate',\n",
       " 'trust',\n",
       " 'blast',\n",
       " 'pnlmtncp',\n",
       " 'weeding',\n",
       " 'newlydisplaced',\n",
       " 'luhanskrussia',\n",
       " 'bouts',\n",
       " 'gawashti',\n",
       " 'seme',\n",
       " 'lowquality',\n",
       " 'cyril',\n",
       " 'understaffed',\n",
       " 'livelihoodsaving',\n",
       " 'government–implemented',\n",
       " 'string',\n",
       " 'programs—representing',\n",
       " 'burkinabé',\n",
       " 'venezuelasince',\n",
       " 'warao',\n",
       " 'strictures',\n",
       " 'vaccinepreventable',\n",
       " 'salvador',\n",
       " 'araya',\n",
       " 'fearing',\n",
       " 'pakhtunkhwa',\n",
       " 'oreamuno',\n",
       " 'snowing',\n",
       " 'childrenthe',\n",
       " 'adverse',\n",
       " 'alqitar',\n",
       " 'shamarekh',\n",
       " 'so',\n",
       " 'riverine',\n",
       " 'don',\n",
       " 'unexpected',\n",
       " 'uprooting',\n",
       " 'defining',\n",
       " 'secretaries',\n",
       " 'jewelry',\n",
       " 'explains',\n",
       " 'chinal',\n",
       " 'bottling',\n",
       " 'lashkarejhangvi',\n",
       " 'dfr',\n",
       " 'inserted',\n",
       " 'stateless',\n",
       " 'foreigners',\n",
       " 'beninese',\n",
       " 'countriesseveral',\n",
       " 'kéréboulé',\n",
       " 'printed',\n",
       " 'sosyan',\n",
       " 'octobertodecember',\n",
       " 'fiscal',\n",
       " 'individuals•',\n",
       " 'facilitybased',\n",
       " 'gita',\n",
       " 'guns',\n",
       " 'nuristan',\n",
       " 'waral',\n",
       " 'rustled',\n",
       " 'spatiotemporal',\n",
       " 'postacute',\n",
       " 'padiet',\n",
       " 'lumombo',\n",
       " 'immaturity',\n",
       " 'menaka',\n",
       " 'cited',\n",
       " 'rewood',\n",
       " 'chumphon',\n",
       " 'hasbeen',\n",
       " 'nabul',\n",
       " 'solomon',\n",
       " 'recommends',\n",
       " 'bpbd',\n",
       " 'notices',\n",
       " 'contractor',\n",
       " 'been',\n",
       " 'registerednew',\n",
       " 'overseas',\n",
       " 'begged',\n",
       " 'mysterious',\n",
       " 'dowa',\n",
       " 'muchena',\n",
       " 'jumhoriya',\n",
       " 'checkposts',\n",
       " 'latilak',\n",
       " 'overlaps',\n",
       " 'overran',\n",
       " 'idps61',\n",
       " 'katrinahanse',\n",
       " 'stilts',\n",
       " 'drc',\n",
       " 'overcrowding',\n",
       " 'destroyed',\n",
       " 'habur',\n",
       " 'branch',\n",
       " 'resisting',\n",
       " 'cruz',\n",
       " 'droughtimpacted',\n",
       " 'violates',\n",
       " 'damage',\n",
       " 'kalobayei',\n",
       " 'electrical',\n",
       " 'nam',\n",
       " 'adding',\n",
       " 'educations',\n",
       " 'narrowed',\n",
       " 'posited',\n",
       " 'abugarda',\n",
       " 'aisladas',\n",
       " 'vehicleborne',\n",
       " 'expresses',\n",
       " 'mexican',\n",
       " 'unfold',\n",
       " 'iomanother',\n",
       " 'detectives',\n",
       " 'aggravating',\n",
       " 'polio',\n",
       " 'morombe',\n",
       " 'didnt',\n",
       " 'pozo',\n",
       " 'bmms',\n",
       " 'buale',\n",
       " 'dignity',\n",
       " 'twapia',\n",
       " 'weeks—or',\n",
       " 'devastated',\n",
       " 'crical',\n",
       " 'bottleneck',\n",
       " 'pared',\n",
       " 'umed',\n",
       " 'planted3',\n",
       " 'behrami',\n",
       " 'busaira',\n",
       " 'trafficable',\n",
       " 'garda',\n",
       " 'pots',\n",
       " 'sanad',\n",
       " 'irathis',\n",
       " 'flooding',\n",
       " 'ising',\n",
       " 'cheruiyot',\n",
       " 'escalating',\n",
       " 'castilla',\n",
       " 'data',\n",
       " 'libiri',\n",
       " 'shelled',\n",
       " 'postebola',\n",
       " 'commissary',\n",
       " 'levelling',\n",
       " 'kings',\n",
       " 'nyanza',\n",
       " 'lassa',\n",
       " 'nus',\n",
       " 'benard',\n",
       " 'launched',\n",
       " 'board—and',\n",
       " 'exemption',\n",
       " 'sneezes',\n",
       " 'huya',\n",
       " 'sanma',\n",
       " 'quinta',\n",
       " 'cilss',\n",
       " 'ibba',\n",
       " 'todayaccording',\n",
       " 'imbalances',\n",
       " 'loc',\n",
       " 'ideam',\n",
       " 'into',\n",
       " 'shargafab',\n",
       " 'hekim',\n",
       " 'gene',\n",
       " 'weak',\n",
       " 'mechanisms',\n",
       " 'im',\n",
       " 'monks',\n",
       " 'cityhe',\n",
       " 'composition',\n",
       " 'regroupment',\n",
       " 'kazungu',\n",
       " 'deep',\n",
       " 'prospect',\n",
       " 'iyire',\n",
       " 'interamerican',\n",
       " 'calls',\n",
       " 'pressured',\n",
       " 'forecast',\n",
       " 'nasr',\n",
       " 'reuse',\n",
       " 'whiston',\n",
       " 'recognizes',\n",
       " 'idlib',\n",
       " 'noua',\n",
       " 'bamzok',\n",
       " 'portoviejo',\n",
       " 'surprisingly',\n",
       " 'sdg350',\n",
       " 'rahana',\n",
       " 'mondays',\n",
       " 'hometowns',\n",
       " 'happens',\n",
       " 'consumes',\n",
       " 'cholerae',\n",
       " 'weathering',\n",
       " 'lands',\n",
       " 'alula',\n",
       " 'ign',\n",
       " 'intense',\n",
       " 'licenses',\n",
       " 'megawatt',\n",
       " 'posing',\n",
       " 'situationmonsur',\n",
       " 'sail',\n",
       " 'crews',\n",
       " 'emanate',\n",
       " 'sim',\n",
       " 'ould',\n",
       " 'statistical',\n",
       " 'opened',\n",
       " 'annihilate',\n",
       " 'townthe',\n",
       " 'panning',\n",
       " 'nangbéto',\n",
       " 'incentive',\n",
       " 'ambitions',\n",
       " 'ojete',\n",
       " 'dphe',\n",
       " 'saidfollowing',\n",
       " 'serbian',\n",
       " 'qatarbased',\n",
       " 'children—the',\n",
       " 'reeling',\n",
       " 'speaking',\n",
       " 'cordone',\n",
       " 'insecurity—or',\n",
       " 'seasonswe',\n",
       " 'destroy',\n",
       " 'difficulty',\n",
       " 'ripe',\n",
       " 'tin',\n",
       " 'dos',\n",
       " 'effected',\n",
       " 'activitydiscussions',\n",
       " 'mariupol',\n",
       " 'jalpa',\n",
       " 'kassab',\n",
       " 'tanzanian',\n",
       " 'titao',\n",
       " 'nd',\n",
       " 'receives',\n",
       " 'njikwa',\n",
       " 'welldistributed',\n",
       " 'speedy',\n",
       " 'statistically',\n",
       " 'hair',\n",
       " 'fromsite',\n",
       " 'indonesian',\n",
       " 'bangladesh',\n",
       " 'apro',\n",
       " 'nears',\n",
       " 'propagating',\n",
       " 'silchar',\n",
       " 'benin',\n",
       " 'distr',\n",
       " 'communitiesthe',\n",
       " 'warsheikh',\n",
       " 'protectionfocused',\n",
       " 'sudanesepound',\n",
       " 'worrisome',\n",
       " 'commanders',\n",
       " 'detection',\n",
       " 'consensus',\n",
       " 'batna',\n",
       " 'gworam',\n",
       " 'bangui',\n",
       " 'pledges',\n",
       " 'polarization',\n",
       " 'daylong',\n",
       " 'fellow',\n",
       " 'octobertomarch',\n",
       " 'truths',\n",
       " 'portrays',\n",
       " 'cultivo',\n",
       " 'ration',\n",
       " 'scrap',\n",
       " 'schools',\n",
       " 'ofra',\n",
       " 'feburary',\n",
       " 'environments•',\n",
       " 'snnpr',\n",
       " 'weekthere',\n",
       " 'facts',\n",
       " 'crisisridden',\n",
       " 'storiesthe',\n",
       " 'comprised',\n",
       " 'xdrtb',\n",
       " 'unsmil',\n",
       " 'governorselect',\n",
       " 'caution',\n",
       " 'isiscontrolled',\n",
       " 'suf',\n",
       " 'unsafequestionable',\n",
       " 'thats',\n",
       " 'thinni',\n",
       " 'unpopular',\n",
       " 'basins',\n",
       " 'persistin',\n",
       " 'ntca',\n",
       " 'kape',\n",
       " 'against',\n",
       " 'rumors',\n",
       " 'otakhonov',\n",
       " 'sad',\n",
       " 'brady',\n",
       " 'bednetstwo',\n",
       " 'fighter',\n",
       " 'rakyat',\n",
       " 'jurisdiction',\n",
       " 'deped',\n",
       " 'ameijeiras',\n",
       " 'focusgroup',\n",
       " 'unsubstantiated',\n",
       " 'highly',\n",
       " 'expected',\n",
       " 'ponds',\n",
       " 'age18',\n",
       " 'nigeriathe',\n",
       " 'alfahd',\n",
       " 'tha',\n",
       " 'bassey',\n",
       " 'kasukuwere',\n",
       " 'stanikzai',\n",
       " 'peopleand',\n",
       " 'raft',\n",
       " 'baja',\n",
       " 'grillet',\n",
       " 'sourcesthis',\n",
       " 'snhr',\n",
       " 'usg',\n",
       " 'retinal',\n",
       " 'ancient',\n",
       " 'amman',\n",
       " 'bulungan',\n",
       " 'forecasting',\n",
       " 'aid',\n",
       " 'misarta',\n",
       " 'mineral',\n",
       " 'faratsiho',\n",
       " 'repairsrehabilitations',\n",
       " 'wing',\n",
       " 'insecurehowever',\n",
       " 'nage',\n",
       " 'donkeys',\n",
       " 'seams',\n",
       " 'malnutrition',\n",
       " 'diyarbakir',\n",
       " 'lowest',\n",
       " 'haramrelated',\n",
       " 'lii',\n",
       " 'faraway',\n",
       " 'waiting',\n",
       " 'slaughter',\n",
       " 'death',\n",
       " 'enabling',\n",
       " 'resiliency',\n",
       " 'dollar',\n",
       " 'liberian',\n",
       " 'spinal',\n",
       " 'bloomberg',\n",
       " 'atrto',\n",
       " 'nomads',\n",
       " 'respiration',\n",
       " 'lunda',\n",
       " 'people1',\n",
       " 'rerouted',\n",
       " 'partners',\n",
       " 'masafer',\n",
       " 'though',\n",
       " 'habeas',\n",
       " 'joachim',\n",
       " 'ized',\n",
       " 'xudur',\n",
       " 'us2billion',\n",
       " 'term',\n",
       " 'rome',\n",
       " 'rotten',\n",
       " 'ffm',\n",
       " 'good',\n",
       " 'higherprevalence',\n",
       " 'tippy',\n",
       " 'exponentially',\n",
       " 'lopa',\n",
       " 'chilli',\n",
       " 'midapril',\n",
       " 'challengesthe',\n",
       " 'hhs',\n",
       " 'vanuatu',\n",
       " 'shon',\n",
       " 'habibo',\n",
       " 'divisive',\n",
       " 'sanipa',\n",
       " 'pumped',\n",
       " 'sociale',\n",
       " 'bangla',\n",
       " 'barricades',\n",
       " 'meanchey',\n",
       " 'elwadi',\n",
       " 'facebook',\n",
       " 'petrovskyi',\n",
       " 'harm',\n",
       " 'ombadda',\n",
       " 'zion',\n",
       " 'cdtr',\n",
       " 'damaged15',\n",
       " 'document2',\n",
       " 'sectarian',\n",
       " 'dundo',\n",
       " 'implies',\n",
       " 'countyat',\n",
       " 'illeret',\n",
       " 'their',\n",
       " 'ede',\n",
       " 'inadequately',\n",
       " 'sack',\n",
       " 'impeller',\n",
       " 'best',\n",
       " 'strip',\n",
       " 'vista',\n",
       " 'foodbased',\n",
       " 'yakovenkove',\n",
       " 'aweil',\n",
       " 'sixtyeight',\n",
       " 'barrages',\n",
       " 'ursula',\n",
       " 'aratuko',\n",
       " 'ses',\n",
       " 'tangulbei',\n",
       " 'encamped',\n",
       " 'gauges',\n",
       " 'interna\\xadtionals',\n",
       " 'hometown',\n",
       " 'vigilant',\n",
       " 'pultupi',\n",
       " 'accidents',\n",
       " 'vakinakaratra',\n",
       " 'connection',\n",
       " 'bongo',\n",
       " 'interdiction',\n",
       " 'pijin',\n",
       " 'pervasive',\n",
       " 'spodoptera',\n",
       " 'mangochi',\n",
       " 'waziristan',\n",
       " 'obock',\n",
       " 'combine',\n",
       " 'aktobe',\n",
       " 'protectionas',\n",
       " 'kailahun',\n",
       " 'xaysdaada',\n",
       " 'approaching',\n",
       " 'garments',\n",
       " 'nisa',\n",
       " 'fijians',\n",
       " 'enshrined',\n",
       " 'controls',\n",
       " 'other',\n",
       " 'over5',\n",
       " 'siraha',\n",
       " 'harar',\n",
       " 'alhadidiya',\n",
       " 'imperative',\n",
       " 'horrific',\n",
       " 'ions',\n",
       " 'prolongation',\n",
       " 'expulsions',\n",
       " 'elisaantigen',\n",
       " 'overuse',\n",
       " 'miwi',\n",
       " 'appearing',\n",
       " 'spillways',\n",
       " 'ely',\n",
       " 'mud',\n",
       " 'knockon',\n",
       " 'mmp',\n",
       " 'committee',\n",
       " 'game',\n",
       " 'cholerathis',\n",
       " 'patriotic',\n",
       " 'memorandum',\n",
       " 'lessen',\n",
       " 'cohorts',\n",
       " 'ytd',\n",
       " 'nonrefugees',\n",
       " 'bad',\n",
       " 'parents',\n",
       " 'bnpb',\n",
       " 'neighbor',\n",
       " 'utmost',\n",
       " 'relaxed',\n",
       " 'doza',\n",
       " 'commercespeaking',\n",
       " 'camajuaní',\n",
       " 'godere',\n",
       " 'parttime',\n",
       " 'violate',\n",
       " 'fraught',\n",
       " 'digging',\n",
       " 'ssgaffiliated',\n",
       " 'union',\n",
       " 'deadliness',\n",
       " 'bsfp',\n",
       " 'assessments',\n",
       " 'constraining',\n",
       " 'marketdanilovich',\n",
       " 'pabulek',\n",
       " 'kaew',\n",
       " 'aquatics',\n",
       " 'simultaneous',\n",
       " 'declares',\n",
       " 'eviction',\n",
       " 'writesunfortunately',\n",
       " 'whipped',\n",
       " 'ipob',\n",
       " 'life5',\n",
       " 'mahrah',\n",
       " 'supported',\n",
       " 'calling',\n",
       " 'ecologically',\n",
       " 'merscov',\n",
       " 'ukhia',\n",
       " 'basir',\n",
       " 'cordoned',\n",
       " 'conception',\n",
       " 'verdicts',\n",
       " 'horrendous',\n",
       " 'animalhuman',\n",
       " 'personhowever',\n",
       " 'samthe',\n",
       " 'operating',\n",
       " 'fuelwood',\n",
       " 'triu',\n",
       " 'participant',\n",
       " 'violent',\n",
       " 'kasai',\n",
       " 'nonlebanese',\n",
       " 'recess',\n",
       " 'crops',\n",
       " 'continuedviolent',\n",
       " 'incredible',\n",
       " 'casebycase',\n",
       " 'arras',\n",
       " 'elimlim',\n",
       " 'hausa',\n",
       " 'kochav',\n",
       " 'unprotectedunsafe',\n",
       " 'azerbaijani',\n",
       " 'rectification',\n",
       " 'tumaco',\n",
       " 'interns',\n",
       " 'prince',\n",
       " 'traded',\n",
       " 'agropastoralist',\n",
       " 'nearby',\n",
       " 'awards',\n",
       " 'me',\n",
       " 'tomb',\n",
       " 'pharmaceutical',\n",
       " 'trumps',\n",
       " 'resent',\n",
       " 'frontier',\n",
       " 'ecards',\n",
       " 'mayreportedly',\n",
       " 'lashkar',\n",
       " 'brace',\n",
       " 'pickers',\n",
       " 'municipalitiescities',\n",
       " 'kote',\n",
       " 'mamenti',\n",
       " 'jandoh',\n",
       " 'occurrence',\n",
       " 'unsurprisingly',\n",
       " 'renewals',\n",
       " 'eightyseven',\n",
       " 'alquesra',\n",
       " 'knowing',\n",
       " 'nptp',\n",
       " 'sarlahi',\n",
       " 'september35',\n",
       " 'traffickers—including',\n",
       " 'teknaf',\n",
       " 'jo',\n",
       " 'liberate',\n",
       " 'pylon',\n",
       " 'multiplicity',\n",
       " 'rustling',\n",
       " 'bombardment',\n",
       " 'surgeries',\n",
       " 'headscarves',\n",
       " 'rioters',\n",
       " 'sebigoro',\n",
       " 'basements',\n",
       " 'burundians',\n",
       " 'anchorage',\n",
       " 'possibilities',\n",
       " 'sws',\n",
       " 'blunt',\n",
       " 'courtyards',\n",
       " 'enjoyment',\n",
       " 'proven',\n",
       " 'nhn',\n",
       " 'janale',\n",
       " 'hepute',\n",
       " 'vahdat',\n",
       " 'lurch',\n",
       " 'unconstitutional',\n",
       " 'mashal',\n",
       " 'hutus',\n",
       " 'commodities—including',\n",
       " 'nambong',\n",
       " 'lingering',\n",
       " 'alspachunorgbruce',\n",
       " 'muruny',\n",
       " 'wellspeaking',\n",
       " 'choke',\n",
       " 'humanity…to',\n",
       " 'maluana',\n",
       " 'bonbon',\n",
       " 'precipitated',\n",
       " 'afdher',\n",
       " 'baranga',\n",
       " 'albaz',\n",
       " 'mile',\n",
       " 'preelimination',\n",
       " 'ihosy',\n",
       " 'bethlehembased',\n",
       " 'lymphadenopathy',\n",
       " 'dwindle',\n",
       " 'egte',\n",
       " 'kebrdehar',\n",
       " 'iycfcontinuous',\n",
       " 'francis',\n",
       " 'massafer',\n",
       " 'outcomesin',\n",
       " 'mangaten',\n",
       " 'surplus',\n",
       " 'fuelling',\n",
       " 'dukhun',\n",
       " 'playersdisruption',\n",
       " 'leastsafe',\n",
       " 'umanitarian',\n",
       " 'pavlo',\n",
       " 'hamdaniya',\n",
       " 'martial',\n",
       " 'ghanimah',\n",
       " 'thrive',\n",
       " 'ranchers',\n",
       " 'lebanese',\n",
       " 'reinstatement',\n",
       " 'sosh',\n",
       " 'smc',\n",
       " 'dollarin',\n",
       " 'hamatos',\n",
       " 'outofschoolchildren',\n",
       " 'interconnected',\n",
       " 'kediba',\n",
       " 'immunising',\n",
       " 'civilianpopulated',\n",
       " 'obstructs',\n",
       " 'aburoch',\n",
       " 'delegation',\n",
       " 'fraction',\n",
       " 'landmind',\n",
       " 'people—in',\n",
       " 'student',\n",
       " 'opecled',\n",
       " 'panel',\n",
       " 'sabe',\n",
       " 'kot',\n",
       " 'hadibo',\n",
       " 'partial',\n",
       " 'rsmc',\n",
       " 'abdin',\n",
       " 'arson',\n",
       " 'mondaywuasa',\n",
       " 'extreme',\n",
       " 'ambo',\n",
       " 'daythe',\n",
       " 'tourism',\n",
       " 'mosw',\n",
       " 'armyworms',\n",
       " 'antofagasta',\n",
       " 'baydoun',\n",
       " 'armm',\n",
       " 'res',\n",
       " 'roberto',\n",
       " 'may–july',\n",
       " 'city',\n",
       " 'discrimination',\n",
       " 'nonlethal',\n",
       " 'ochahno',\n",
       " 'biliran',\n",
       " 'progressive',\n",
       " 'fridaydrought',\n",
       " 'ficked',\n",
       " 'abyei',\n",
       " 'investigators',\n",
       " 'certain',\n",
       " 'hungary',\n",
       " 'overcrowded',\n",
       " 'ofer',\n",
       " 'abim',\n",
       " 'urgency',\n",
       " 'destination',\n",
       " 'shelf',\n",
       " 'canelones',\n",
       " 'disorder',\n",
       " 'tierralta',\n",
       " 'monseñor',\n",
       " 'fortyfour',\n",
       " 'washouts',\n",
       " 'antillegal',\n",
       " 'bartalla',\n",
       " 'housed',\n",
       " 'testingmapping',\n",
       " 'phenomena',\n",
       " 'zanu',\n",
       " 'daghr',\n",
       " 'enoughayham',\n",
       " 'owusu',\n",
       " 'convictions',\n",
       " 'projected',\n",
       " 'collateral',\n",
       " 'sheikan',\n",
       " 'morobo',\n",
       " 'bordering',\n",
       " 'incharge',\n",
       " 'golweyn',\n",
       " 'sog',\n",
       " 'ghassan',\n",
       " 'bred',\n",
       " 'cleaners',\n",
       " 'campin',\n",
       " 'waterstressed',\n",
       " 'coldstorage',\n",
       " 'sdhs',\n",
       " 'infrastructures',\n",
       " 'gebran',\n",
       " 'risto',\n",
       " 'unbundle',\n",
       " 'tasks',\n",
       " 'nightshift',\n",
       " 'gasseer',\n",
       " 'lifelong',\n",
       " 'adar',\n",
       " 'cerrejon',\n",
       " 'noone',\n",
       " 'chronic',\n",
       " 'plagued',\n",
       " 'spi',\n",
       " 'mechanismsin',\n",
       " 'sow',\n",
       " 'volume',\n",
       " 'alshababrelated',\n",
       " 'nowdisarmed',\n",
       " 'cfrthe',\n",
       " 'period',\n",
       " 'alarms',\n",
       " 'zvulun',\n",
       " 'separations',\n",
       " 'terroristrelated',\n",
       " 'landlevelling',\n",
       " 'zayed',\n",
       " 'surfacetoair',\n",
       " 'zinc',\n",
       " 'caves',\n",
       " 'sduit',\n",
       " 'dhaele',\n",
       " 'tanga',\n",
       " 'clashing',\n",
       " 'crises',\n",
       " 'penh',\n",
       " 'hatra',\n",
       " 'spotbandit',\n",
       " 'tombouctou',\n",
       " 'vent',\n",
       " 'marondera',\n",
       " 'corpus',\n",
       " 'mother',\n",
       " 'purani',\n",
       " 'achieve',\n",
       " 'jeda',\n",
       " 'affirmed',\n",
       " 'romeo',\n",
       " 'fares',\n",
       " 'artillery',\n",
       " 'hosts',\n",
       " 'almonds',\n",
       " 'kanyoni',\n",
       " 'malaybalay',\n",
       " 'ivanofrankivsk',\n",
       " 'marketdependent',\n",
       " 'unfor',\n",
       " 'wollega',\n",
       " 'cassation',\n",
       " 'eecps',\n",
       " 'creating',\n",
       " 'malta',\n",
       " 'necessitated',\n",
       " 'saraqeb',\n",
       " 'plateaus',\n",
       " 'casting',\n",
       " 'deteriorated',\n",
       " 'peoplewfp',\n",
       " 'op',\n",
       " 'salahadeen',\n",
       " 'watersharing',\n",
       " 'occupancy',\n",
       " 'trebinjebileca',\n",
       " 'marchspeaking',\n",
       " 'nino',\n",
       " 'takes',\n",
       " 'nrl',\n",
       " 'twoyearold',\n",
       " 'rmms',\n",
       " 'islamkot',\n",
       " 'proliferated',\n",
       " 'hiraanin',\n",
       " 'pioneers',\n",
       " 'barley',\n",
       " 'workshops',\n",
       " 'evacuation',\n",
       " 'mirrors',\n",
       " 'agitating',\n",
       " 'assignments',\n",
       " 'chilena',\n",
       " 'academics',\n",
       " 'soavina',\n",
       " 'tibiri',\n",
       " 'shaykhun',\n",
       " 'encountering',\n",
       " 'fican',\n",
       " 'sitescamps',\n",
       " 'salamat',\n",
       " 'buurdhuxule',\n",
       " 'rapporteurs',\n",
       " 'cfr',\n",
       " 'congolese',\n",
       " 'message',\n",
       " 'micky',\n",
       " 'depletes',\n",
       " 'exempting',\n",
       " 'defeat',\n",
       " 'sundaythey',\n",
       " 'owdai',\n",
       " 'guyo',\n",
       " 'basethe',\n",
       " 'den2',\n",
       " 'upswing',\n",
       " 'jeddah',\n",
       " 'fivepoint',\n",
       " 'kamayamaa',\n",
       " 'nuts',\n",
       " 'mehman',\n",
       " 'shahid',\n",
       " 'methodology',\n",
       " 'gangofficials',\n",
       " 'posting',\n",
       " 'inconvenience',\n",
       " 'ze',\n",
       " 'por',\n",
       " 'banauehungduanbenguet',\n",
       " 'ensure',\n",
       " 'divided',\n",
       " 'slumps',\n",
       " 'nonfood',\n",
       " 'saharan',\n",
       " 'malaysian',\n",
       " 'clearingup',\n",
       " 'lawlessness',\n",
       " 'sink',\n",
       " 'availabilitybakeries',\n",
       " 'assisting',\n",
       " 'usd150',\n",
       " 'iscontrol',\n",
       " 'scant',\n",
       " 'regu',\n",
       " 'evening',\n",
       " 'contention',\n",
       " 'enlarged',\n",
       " 'kenyaâs',\n",
       " 'extremists',\n",
       " 'assay',\n",
       " 'pudahuel',\n",
       " 'bakers',\n",
       " 'palestinian',\n",
       " 'facing',\n",
       " 'defecating',\n",
       " 'second',\n",
       " 'koolivand',\n",
       " 'ast',\n",
       " 'mountainous',\n",
       " 'samarasinghe',\n",
       " 'elder',\n",
       " 'cra',\n",
       " 'marrahowever',\n",
       " 'nails',\n",
       " 'children—who',\n",
       " 'pokot',\n",
       " 'flashflood',\n",
       " 'deepened',\n",
       " 'provis',\n",
       " 'catastrophe2',\n",
       " 'disabilities',\n",
       " 'mahmud',\n",
       " 'rohingyan',\n",
       " 'syrianjordanian',\n",
       " 'lpdin',\n",
       " 'ansarul',\n",
       " 'almasara',\n",
       " 'fidunicefworld',\n",
       " 'mwk',\n",
       " 'rundown',\n",
       " 'longerterm',\n",
       " 'valuable',\n",
       " 'mabalane',\n",
       " 'uninhibited',\n",
       " 'lawsofwar',\n",
       " 'lamb',\n",
       " 'regional',\n",
       " 'officer',\n",
       " 'impassible',\n",
       " 'alwaral',\n",
       " 'named',\n",
       " 'powdered',\n",
       " 'shantmanu',\n",
       " 'niñoinduced',\n",
       " 'unlock',\n",
       " ...}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 47483)\n",
      "('of', 34072)\n",
      "('in', 30424)\n",
      "('and', 29129)\n",
      "('to', 23147)\n",
      "('a', 10243)\n",
      "('are', 7708)\n",
      "('for', 6721)\n",
      "('is', 6509)\n",
      "('have', 6020)\n"
     ]
    }
   ],
   "source": [
    "# sorting by count: \n",
    "sorted_list = sorted(train_wordcount.items(), key=lambda item: item[1],reverse = True)\n",
    "sorted_vocab = dict(sorted_list)\n",
    "\n",
    "#print out the 10 most frequent words:\n",
    "for i in range(10): \n",
    "    print(sorted_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#try lemmatization with spacy:\n",
    "import spacy\n",
    "lem = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "lem_tokens_spacy = set()\n",
    "train_vectors = []\n",
    "\n",
    "for token in train_dictionary:\n",
    "    lem_tokens_spacy.add (lem(token)[0].lemma_ ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25548"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lem_tokens_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'leishmaniosis',\n",
       " 'twentythree',\n",
       " 'mathow',\n",
       " 'spe',\n",
       " 'mughayyir',\n",
       " 'napalmlike',\n",
       " 'mentao',\n",
       " 'copán',\n",
       " 'ein',\n",
       " 'swedisheritrean',\n",
       " 'oppression',\n",
       " 'women53',\n",
       " 'rumek',\n",
       " 'radio',\n",
       " 'resumption',\n",
       " 'us85',\n",
       " 'cohort',\n",
       " 'pulse',\n",
       " 'akech',\n",
       " 'same',\n",
       " 'closeknit',\n",
       " 'com\\xadm\\xadissio\\xadner',\n",
       " 'churoamaya',\n",
       " 'mobilise',\n",
       " 'landmark',\n",
       " 'literally',\n",
       " 'maoist',\n",
       " 'schoolthe',\n",
       " 'just',\n",
       " 'shomolu',\n",
       " 'mogadisho',\n",
       " 'sanabel',\n",
       " 'flurry',\n",
       " 'kaskazini',\n",
       " 'straggler',\n",
       " 'vieja',\n",
       " 'supportwhile',\n",
       " 'sheikhoun',\n",
       " 'obeid',\n",
       " 'baracoa',\n",
       " 'prang',\n",
       " 'beheloka',\n",
       " 'osun',\n",
       " 'lokshina',\n",
       " 'unprecedented',\n",
       " 'atatrah',\n",
       " 'suriname',\n",
       " 'manofoli',\n",
       " 'calve',\n",
       " 'sittwe',\n",
       " 'andunclean',\n",
       " 'bate',\n",
       " 'trust',\n",
       " 'blast',\n",
       " 'pnlmtncp',\n",
       " 'newlydisplaced',\n",
       " 'luhanskrussia',\n",
       " 'gawashti',\n",
       " 'seme',\n",
       " 'illequippe',\n",
       " 'lowquality',\n",
       " 'warrelate',\n",
       " 'cyril',\n",
       " 'string',\n",
       " 'burkinabé',\n",
       " 'venezuelasince',\n",
       " 'warao',\n",
       " 'youthage',\n",
       " 'emirate',\n",
       " 'vaccinepreventable',\n",
       " 'salvador',\n",
       " 'araya',\n",
       " 'pakhtunkhwa',\n",
       " 'oreamuno',\n",
       " 'childrenthe',\n",
       " 'adverse',\n",
       " 'alqitar',\n",
       " 'shamarekh',\n",
       " 'so',\n",
       " 'riverine',\n",
       " 'don',\n",
       " 'unexpected',\n",
       " 'illprepare',\n",
       " 'jewelry',\n",
       " 'chinal',\n",
       " 'lashkarejhangvi',\n",
       " 'dfr',\n",
       " 'stateless',\n",
       " 'beninese',\n",
       " 'countriesseveral',\n",
       " 'kéréboulé',\n",
       " 'sosyan',\n",
       " 'octobertodecember',\n",
       " 'fiscal',\n",
       " 'individuals•',\n",
       " 'facilitybased',\n",
       " 'gita',\n",
       " 'nuristan',\n",
       " 'pause',\n",
       " 'nonconflictaffecte',\n",
       " 'waral',\n",
       " 'spatiotemporal',\n",
       " 'postacute',\n",
       " 'padiet',\n",
       " 'lumombo',\n",
       " 'reassurance',\n",
       " 'immaturity',\n",
       " 'menaka',\n",
       " 'rewood',\n",
       " 'chumphon',\n",
       " 'hasbeen',\n",
       " 'nabul',\n",
       " 'solomon',\n",
       " 'bpbd',\n",
       " 'tubewell',\n",
       " 'contractor',\n",
       " 'registerednew',\n",
       " 'overseas',\n",
       " 'mysterious',\n",
       " 'dowa',\n",
       " 'muchena',\n",
       " 'jumhoriya',\n",
       " 'latilak',\n",
       " 'overran',\n",
       " 'idps61',\n",
       " 'katrinahanse',\n",
       " 'drc',\n",
       " 'habur',\n",
       " 'branch',\n",
       " 'ageset',\n",
       " 'cruz',\n",
       " 'damage',\n",
       " 'kalobayei',\n",
       " 'electrical',\n",
       " 'nam',\n",
       " 'abugarda',\n",
       " 'vehicleborne',\n",
       " 'mexican',\n",
       " 'unfold',\n",
       " 'iomanother',\n",
       " 'polio',\n",
       " 'morombe',\n",
       " 'pozo',\n",
       " 'bmms',\n",
       " 'buale',\n",
       " 'dignity',\n",
       " 'extinguish',\n",
       " 'bandage',\n",
       " 'twapia',\n",
       " 'crical',\n",
       " 'bottleneck',\n",
       " 'brandish',\n",
       " 'umed',\n",
       " 'planted3',\n",
       " 'behrami',\n",
       " 'busaira',\n",
       " 'trafficable',\n",
       " 'garda',\n",
       " 'sanad',\n",
       " 'irathis',\n",
       " 'flooding',\n",
       " 'cheruiyot',\n",
       " 'castilla',\n",
       " 'observer',\n",
       " 'libiri',\n",
       " 'love',\n",
       " 'postebola',\n",
       " 'commissary',\n",
       " 'nyanza',\n",
       " 'lassa',\n",
       " 'benard',\n",
       " 'exemption',\n",
       " 'floodswater',\n",
       " 'huya',\n",
       " 'sanma',\n",
       " 'quinta',\n",
       " 'cilss',\n",
       " 'ibba',\n",
       " 'loc',\n",
       " 'israeliimpose',\n",
       " 'ideam',\n",
       " 'into',\n",
       " 'shargafab',\n",
       " 'hekim',\n",
       " 'hamasgoverne',\n",
       " 'flame',\n",
       " 'gene',\n",
       " 'weak',\n",
       " 'im',\n",
       " 'cityhe',\n",
       " 'composition',\n",
       " 'regroupment',\n",
       " 'kazungu',\n",
       " 'deep',\n",
       " 'prospect',\n",
       " 'iyire',\n",
       " 'interamerican',\n",
       " 'forecast',\n",
       " 'sop',\n",
       " 'nasr',\n",
       " 'reuse',\n",
       " 'whiston',\n",
       " 'idlib',\n",
       " 'noua',\n",
       " 'bamzok',\n",
       " 'portoviejo',\n",
       " 'surprisingly',\n",
       " 'sdg350',\n",
       " 'rahana',\n",
       " 'cholerae',\n",
       " 'alula',\n",
       " 'megawatt',\n",
       " 'childreneffort',\n",
       " 'ign',\n",
       " 'intense',\n",
       " 'situationmonsur',\n",
       " 'sail',\n",
       " 'emanate',\n",
       " 'sim',\n",
       " 'toss',\n",
       " 'verifiedinvestigate',\n",
       " 'ould',\n",
       " 'statistical',\n",
       " 'annihilate',\n",
       " 'tokenbase',\n",
       " 'townthe',\n",
       " 'timesfisherman',\n",
       " 'nangbéto',\n",
       " 'incentive',\n",
       " 'ojete',\n",
       " 'dphe',\n",
       " 'saidfollowing',\n",
       " 'serbian',\n",
       " 'cordone',\n",
       " 'seasonswe',\n",
       " 'destroy',\n",
       " 'difficulty',\n",
       " 'ripe',\n",
       " 'tin',\n",
       " 'nitrate',\n",
       " 'mariupol',\n",
       " 'jalpa',\n",
       " 'kassab',\n",
       " 'tanzanian',\n",
       " 'titao',\n",
       " 'changeneglecte',\n",
       " 'nd',\n",
       " 'ncdcfmoha',\n",
       " 'njikwa',\n",
       " 'welldistributed',\n",
       " 'speedy',\n",
       " 'statistically',\n",
       " 'hair',\n",
       " 'fromsite',\n",
       " 'womenheade',\n",
       " 'indonesian',\n",
       " 'bangladesh',\n",
       " 'apro',\n",
       " 'eld',\n",
       " 'silchar',\n",
       " 'benin',\n",
       " 'distr',\n",
       " 'communitiesthe',\n",
       " 'warsheikh',\n",
       " 'sudanesepound',\n",
       " 'worrisome',\n",
       " 'detection',\n",
       " 'consensus',\n",
       " 'batna',\n",
       " 'gworam',\n",
       " 'bangui',\n",
       " 'polarization',\n",
       " 'daylong',\n",
       " 'fellow',\n",
       " 'octobertomarch',\n",
       " 'cultivo',\n",
       " 'ration',\n",
       " 'scrap',\n",
       " 'rescuer',\n",
       " 'ofra',\n",
       " 'feburary',\n",
       " 'environments•',\n",
       " 'snnpr',\n",
       " 'weekthere',\n",
       " 'betterequippe',\n",
       " 'crisisridden',\n",
       " 'storiesthe',\n",
       " 'xdrtb',\n",
       " 'unsmil',\n",
       " 'governorselect',\n",
       " 'caution',\n",
       " 'suf',\n",
       " 'unsafequestionable',\n",
       " 'thinni',\n",
       " 'unpopular',\n",
       " 'persistin',\n",
       " 'demarcationname',\n",
       " 'ntca',\n",
       " 'kape',\n",
       " 'against',\n",
       " 'lighterskinne',\n",
       " 'cfroutbreak',\n",
       " 'otakhonov',\n",
       " 'brady',\n",
       " 'sad',\n",
       " 'bednetstwo',\n",
       " 'fighter',\n",
       " 'rakyat',\n",
       " 'jurisdiction',\n",
       " 'ameijeiras',\n",
       " 'focusgroup',\n",
       " 'unsubstantiated',\n",
       " 'highly',\n",
       " 'age18',\n",
       " 'nigeriathe',\n",
       " 'alfahd',\n",
       " 'tha',\n",
       " 'backer',\n",
       " 'northward',\n",
       " 'bassey',\n",
       " 'kasukuwere',\n",
       " 'stanikzai',\n",
       " 'peopleand',\n",
       " 'raft',\n",
       " 'baja',\n",
       " 'grillet',\n",
       " 'sourcesthis',\n",
       " 'snhr',\n",
       " 'usg',\n",
       " 'retinal',\n",
       " 'ancient',\n",
       " 'faece',\n",
       " 'amman',\n",
       " 'bulungan',\n",
       " 'aid',\n",
       " 'misarta',\n",
       " 'mineral',\n",
       " 'faratsiho',\n",
       " 'wing',\n",
       " 'insecurehowever',\n",
       " 'nage',\n",
       " 'submerge',\n",
       " 'malnutrition',\n",
       " 'diyarbakir',\n",
       " 'lii',\n",
       " 'faraway',\n",
       " 'slaughter',\n",
       " 'death',\n",
       " 'resiliency',\n",
       " 'dollar',\n",
       " 'liberian',\n",
       " 'spinal',\n",
       " 'bloomberg',\n",
       " 'atrto',\n",
       " 'respiration',\n",
       " 'knee',\n",
       " 'lunda',\n",
       " 'people1',\n",
       " 'inventory',\n",
       " 'masafer',\n",
       " 'though',\n",
       " 'joachim',\n",
       " 'xudur',\n",
       " 'us2billion',\n",
       " 'term',\n",
       " 'rome',\n",
       " 'rotten',\n",
       " 'ffm',\n",
       " 'good',\n",
       " 'higherprevalence',\n",
       " 'tippy',\n",
       " 'exponentially',\n",
       " 'lopa',\n",
       " 'chilli',\n",
       " 'midapril',\n",
       " 'challengesthe',\n",
       " 'hhs',\n",
       " 'vanuatu',\n",
       " 'shon',\n",
       " 'habibo',\n",
       " 'divisive',\n",
       " 'sanipa',\n",
       " 'sociale',\n",
       " 'bangla',\n",
       " 'meanchey',\n",
       " 'elwadi',\n",
       " 'facebook',\n",
       " 'petrovskyi',\n",
       " 'harm',\n",
       " 'ombadda',\n",
       " 'zion',\n",
       " 'cdtr',\n",
       " 'damaged15',\n",
       " 'document2',\n",
       " 'sectarian',\n",
       " 'dundo',\n",
       " 'countyat',\n",
       " 'illeret',\n",
       " 'their',\n",
       " 'ede',\n",
       " 'inadequately',\n",
       " 'sack',\n",
       " 'impeller',\n",
       " 'strip',\n",
       " 'vista',\n",
       " 'yakovenkove',\n",
       " 'aweil',\n",
       " 'sixtyeight',\n",
       " 'ursula',\n",
       " 'auditor',\n",
       " 'aratuko',\n",
       " 'tangulbei',\n",
       " 'hometown',\n",
       " 'vigilant',\n",
       " 'pultupi',\n",
       " 'haramcontrolle',\n",
       " 'vakinakaratra',\n",
       " 'connection',\n",
       " 'bongo',\n",
       " 'interdiction',\n",
       " 'pijin',\n",
       " 'pervasive',\n",
       " 'spodoptera',\n",
       " 'mangochi',\n",
       " 'waziristan',\n",
       " 'paowne',\n",
       " 'obock',\n",
       " 'combine',\n",
       " 'aktobe',\n",
       " 'tajurabase',\n",
       " 'quote',\n",
       " 'kailahun',\n",
       " 'casespatient',\n",
       " 'xaysdaada',\n",
       " 'nisa',\n",
       " 'siraha',\n",
       " 'over5',\n",
       " 'other',\n",
       " 'harar',\n",
       " 'alhadidiya',\n",
       " 'chontale',\n",
       " 'imperative',\n",
       " 'horrific',\n",
       " 'prolongation',\n",
       " 'excombatant',\n",
       " 'elisaantigen',\n",
       " 'washingtonbacke',\n",
       " 'overuse',\n",
       " 'miwi',\n",
       " 'ely',\n",
       " 'mud',\n",
       " 'knockon',\n",
       " 'widemouthe',\n",
       " 'mmp',\n",
       " 'committee',\n",
       " 'game',\n",
       " 'cholerathis',\n",
       " 'patriotic',\n",
       " 'memorandum',\n",
       " 'lessen',\n",
       " 'ytd',\n",
       " 'nonrefugees',\n",
       " 'bad',\n",
       " 'bnpb',\n",
       " 'neighbor',\n",
       " 'utmost',\n",
       " 'relaxed',\n",
       " 'doza',\n",
       " 'camajuaní',\n",
       " 'godere',\n",
       " 'parttime',\n",
       " 'violate',\n",
       " 'fraught',\n",
       " 'union',\n",
       " 'deadliness',\n",
       " 'bsfp',\n",
       " 'marketdanilovich',\n",
       " 'pabulek',\n",
       " 'civiliansinclude',\n",
       " 'kaew',\n",
       " 'groan',\n",
       " 'simultaneous',\n",
       " 'nafee',\n",
       " 'eviction',\n",
       " 'writesunfortunately',\n",
       " 'ipob',\n",
       " 'life5',\n",
       " 'mahrah',\n",
       " 'ecologically',\n",
       " 'merscov',\n",
       " 'ukhia',\n",
       " 'basir',\n",
       " 'app',\n",
       " 'conception',\n",
       " 'horrendous',\n",
       " 'animalhuman',\n",
       " 'personhowever',\n",
       " 'samthe',\n",
       " 'fuelwood',\n",
       " 'triu',\n",
       " 'participant',\n",
       " 'violent',\n",
       " 'kasai',\n",
       " 'nonlebanese',\n",
       " 'recess',\n",
       " 'continuedviolent',\n",
       " 'incredible',\n",
       " 'casebycase',\n",
       " 'hausa',\n",
       " 'elimlim',\n",
       " 'kochav',\n",
       " 'unprotectedunsafe',\n",
       " 'azerbaijani',\n",
       " 'rectification',\n",
       " 'tumaco',\n",
       " 'prince',\n",
       " 'agropastoralist',\n",
       " 'nearby',\n",
       " 'me',\n",
       " 'tomb',\n",
       " 'ferment',\n",
       " 'pharmaceutical',\n",
       " 'resent',\n",
       " 'frontier',\n",
       " 'mayreportedly',\n",
       " 'lashkar',\n",
       " 'brace',\n",
       " 'kote',\n",
       " 'mamenti',\n",
       " 'jandoh',\n",
       " 'occurrence',\n",
       " 'unsurprisingly',\n",
       " 'eightyseven',\n",
       " 'alquesra',\n",
       " 'nptp',\n",
       " 'intertwine',\n",
       " 'sarlahi',\n",
       " 'september35',\n",
       " 'teknaf',\n",
       " 'jo',\n",
       " 'liberate',\n",
       " 'pylon',\n",
       " 'multiplicity',\n",
       " 'bombardment',\n",
       " 'sebigoro',\n",
       " 'burundians',\n",
       " 'anchorage',\n",
       " 'sws',\n",
       " 'blunt',\n",
       " 'enjoyment',\n",
       " 'nhn',\n",
       " 'janale',\n",
       " 'hepute',\n",
       " 'vahdat',\n",
       " 'lurch',\n",
       " 'reportedunite',\n",
       " 'unconstitutional',\n",
       " 'mashal',\n",
       " 'hutus',\n",
       " 'nambong',\n",
       " 'muruny',\n",
       " 'alspachunorgbruce',\n",
       " 'choke',\n",
       " 'maluana',\n",
       " 'bonbon',\n",
       " 'afdher',\n",
       " 'baranga',\n",
       " 'albaz',\n",
       " 'mile',\n",
       " 'preelimination',\n",
       " 'ihosy',\n",
       " 'lymphadenopathy',\n",
       " 'dwindle',\n",
       " 'egte',\n",
       " 'kebrdehar',\n",
       " 'iycfcontinuous',\n",
       " 'francis',\n",
       " 'massafer',\n",
       " 'outcomesin',\n",
       " 'mangaten',\n",
       " 'surplus',\n",
       " 'playersdisruption',\n",
       " 'dukhun',\n",
       " 'leastsafe',\n",
       " 'umanitarian',\n",
       " 'pavlo',\n",
       " 'hamdaniya',\n",
       " 'martial',\n",
       " 'ghanimah',\n",
       " 'barrage',\n",
       " 'thrive',\n",
       " 'instruct',\n",
       " 'lebanese',\n",
       " 'reinstatement',\n",
       " 'sosh',\n",
       " 'smc',\n",
       " 'dollarin',\n",
       " 'outofschoolchildren',\n",
       " 'nubian',\n",
       " 'kediba',\n",
       " 'aburoch',\n",
       " 'delegation',\n",
       " 'fraction',\n",
       " 'landmind',\n",
       " 'student',\n",
       " 'panel',\n",
       " 'sabe',\n",
       " 'kot',\n",
       " 'hadibo',\n",
       " 'partial',\n",
       " 'rsmc',\n",
       " 'abdin',\n",
       " 'arson',\n",
       " 'ngoconstructe',\n",
       " 'mondaywuasa',\n",
       " 'extreme',\n",
       " 'ambo',\n",
       " 'daythe',\n",
       " 'tourism',\n",
       " 'mosw',\n",
       " 'clump',\n",
       " 'antofagasta',\n",
       " 'zimbabwethousand',\n",
       " 'baydoun',\n",
       " 'armm',\n",
       " 'roberto',\n",
       " 'city',\n",
       " 'malibase',\n",
       " 'discrimination',\n",
       " 'nonlethal',\n",
       " 'ochahno',\n",
       " 'biliran',\n",
       " 'progressive',\n",
       " 'fridaydrought',\n",
       " 'abyei',\n",
       " 'certain',\n",
       " 'hungary',\n",
       " 'overcrowded',\n",
       " 'ofer',\n",
       " 'abim',\n",
       " 'urgency',\n",
       " 'destination',\n",
       " 'shelf',\n",
       " 'stateaccorde',\n",
       " 'disorder',\n",
       " 'tierralta',\n",
       " 'monseñor',\n",
       " 'fortyfour',\n",
       " 'antillegal',\n",
       " 'bartalla',\n",
       " 'phenomena',\n",
       " 'zanu',\n",
       " 'daghr',\n",
       " 'enoughayham',\n",
       " 'owusu',\n",
       " 'collateral',\n",
       " 'ferre',\n",
       " 'sheikan',\n",
       " 'morobo',\n",
       " 'incharge',\n",
       " 'golweyn',\n",
       " 'sog',\n",
       " 'ghassan',\n",
       " 'campin',\n",
       " 'coldstorage',\n",
       " 'sdhs',\n",
       " 'gebran',\n",
       " 'risto',\n",
       " 'unbundle',\n",
       " 'nightshift',\n",
       " 'fencefire',\n",
       " 'gasseer',\n",
       " 'lifelong',\n",
       " 'adar',\n",
       " 'cerrejon',\n",
       " 'noone',\n",
       " 'chronic',\n",
       " 'spi',\n",
       " 'mechanismsin',\n",
       " 'sow',\n",
       " 'volume',\n",
       " 'cfrthe',\n",
       " 'period',\n",
       " 'zvulun',\n",
       " 'surfacetoair',\n",
       " 'zinc',\n",
       " 'sduit',\n",
       " 'dhaele',\n",
       " 'prosper',\n",
       " 'billow',\n",
       " 'tanga',\n",
       " 'penh',\n",
       " 'hatra',\n",
       " 'spotbandit',\n",
       " 'tombouctou',\n",
       " 'vent',\n",
       " 'marondera',\n",
       " 'corpus',\n",
       " 'mother',\n",
       " 'purani',\n",
       " 'achieve',\n",
       " 'jeda',\n",
       " 'romeo',\n",
       " 'artillery',\n",
       " 'kanyoni',\n",
       " 'malaybalay',\n",
       " 'ivanofrankivsk',\n",
       " 'marketdependent',\n",
       " 'unfor',\n",
       " 'departmentsign',\n",
       " 'wollega',\n",
       " 'cassation',\n",
       " 'formulate',\n",
       " 'malta',\n",
       " 'saraqeb',\n",
       " 'plateaus',\n",
       " 'peoplewfp',\n",
       " 'op',\n",
       " 'salahadeen',\n",
       " 'trebinjebileca',\n",
       " 'occupancy',\n",
       " 'fingerprint',\n",
       " 'nino',\n",
       " 'nrl',\n",
       " 'twoyearold',\n",
       " 'rmms',\n",
       " 'islamkot',\n",
       " 'borderland',\n",
       " 'hiraanin',\n",
       " 'pitch',\n",
       " 'barley',\n",
       " 'evacuation',\n",
       " 'chilena',\n",
       " 'soavina',\n",
       " 'tibiri',\n",
       " 'shaykhun',\n",
       " 'aede',\n",
       " 'livelihoodsave',\n",
       " 'fican',\n",
       " 'salamat',\n",
       " 'buurdhuxule',\n",
       " 'cfr',\n",
       " 'congolese',\n",
       " 'message',\n",
       " 'micky',\n",
       " 'defeat',\n",
       " 'sundaythey',\n",
       " 'owdai',\n",
       " 'guyo',\n",
       " 'basethe',\n",
       " 'den2',\n",
       " 'jeddah',\n",
       " 'fivepoint',\n",
       " 'kamayamaa',\n",
       " 'mehman',\n",
       " 'shahid',\n",
       " 'methodology',\n",
       " 'inconvenience',\n",
       " 'ze',\n",
       " 'por',\n",
       " 'banauehungduanbenguet',\n",
       " 'ensure',\n",
       " 'nonfood',\n",
       " 'saharan',\n",
       " 'malaysian',\n",
       " 'clearingup',\n",
       " 'lawlessness',\n",
       " 'sink',\n",
       " 'usd150',\n",
       " 'iscontrol',\n",
       " 'scant',\n",
       " 'regu',\n",
       " 'evening',\n",
       " 'contention',\n",
       " 'kenyaâs',\n",
       " 'assay',\n",
       " 'pudahuel',\n",
       " 'palestinian',\n",
       " 'second',\n",
       " 'koolivand',\n",
       " 'ast',\n",
       " 'mountainous',\n",
       " 'samarasinghe',\n",
       " 'cra',\n",
       " 'elder',\n",
       " 'marrahowever',\n",
       " 'pokot',\n",
       " 'flashflood',\n",
       " 'provis',\n",
       " 'catastrophe2',\n",
       " 'rohingyan',\n",
       " 'mahmud',\n",
       " 'syrianjordanian',\n",
       " 'lpdin',\n",
       " 'ansarul',\n",
       " 'almasara',\n",
       " 'fidunicefworld',\n",
       " 'mwk',\n",
       " 'rundown',\n",
       " 'longerterm',\n",
       " 'valuable',\n",
       " 'mabalane',\n",
       " 'lawsofwar',\n",
       " 'lamb',\n",
       " 'regional',\n",
       " 'sling',\n",
       " 'officer',\n",
       " 'impassible',\n",
       " 'alwaral',\n",
       " 'washroom',\n",
       " 'shantmanu',\n",
       " 'mercede',\n",
       " 'crossroad',\n",
       " 'outlive',\n",
       " 'unlock',\n",
       " 'aquila',\n",
       " 'unsolved',\n",
       " 'uttaradit',\n",
       " 'huseynov',\n",
       " 'complain',\n",
       " 'berhane',\n",
       " 'helena',\n",
       " 'agrosector',\n",
       " 'greenhouse',\n",
       " 'hrw',\n",
       " 'noteworthly',\n",
       " 'productionthe',\n",
       " 'origin',\n",
       " 'croatia',\n",
       " 'capacities9',\n",
       " 'chibabava',\n",
       " 'significance',\n",
       " 'secretarygeneral',\n",
       " 'sdg19',\n",
       " 'corridorthe',\n",
       " 'kallo',\n",
       " 'reformatory',\n",
       " 'chatchai',\n",
       " 'usual',\n",
       " 'aqueduct',\n",
       " 'mitiku',\n",
       " 'baracoain',\n",
       " 'ebola',\n",
       " 'marcha',\n",
       " 'samoa',\n",
       " 'bake',\n",
       " 'severely',\n",
       " 'nightfalladam',\n",
       " 'kaloleni',\n",
       " 'penitentiary',\n",
       " 'farshaya',\n",
       " 'milletsorghum',\n",
       " 'gihanga',\n",
       " 'mangoro',\n",
       " 'seismology',\n",
       " 'reconstruct',\n",
       " 'unresthit',\n",
       " 'ndoh',\n",
       " 'mail',\n",
       " 'kaura',\n",
       " 'dinar',\n",
       " 'charles',\n",
       " 'sardine',\n",
       " 'warrior',\n",
       " 'sfp',\n",
       " 'hespress',\n",
       " 'onfirmed',\n",
       " 'killi',\n",
       " 'inhumane',\n",
       " 'gna',\n",
       " 'unit',\n",
       " 'quettakandahar',\n",
       " 'gulnora',\n",
       " 'gakara',\n",
       " 'minusma',\n",
       " 'press',\n",
       " 'arori',\n",
       " 'bidi',\n",
       " 'breast',\n",
       " 'sanctuary',\n",
       " 'selfmade',\n",
       " 'polling',\n",
       " 'soroa',\n",
       " 'mzn3200',\n",
       " 'tomás',\n",
       " 'midmay',\n",
       " 'christopher',\n",
       " 'typical',\n",
       " 'fishing',\n",
       " 'recount',\n",
       " 'marsin',\n",
       " 'flulike',\n",
       " 'ously',\n",
       " 'diba',\n",
       " 'cobble',\n",
       " 'bankscrop',\n",
       " 'lowvalue',\n",
       " 'attacker',\n",
       " 'repetition',\n",
       " 'squatter',\n",
       " 'kigoma',\n",
       " 'cucutabut',\n",
       " 'agree',\n",
       " 'ever',\n",
       " 'twgs',\n",
       " 'rst',\n",
       " 'confirmedprobable',\n",
       " 'decent',\n",
       " 'disappointed',\n",
       " 'uproot',\n",
       " 'intimidation',\n",
       " 'ihmayyer',\n",
       " 'saidbangladesh',\n",
       " 'balyun',\n",
       " 'topple',\n",
       " 'bangladeshwe',\n",
       " 'arrivalsassessment',\n",
       " 'mouzalas',\n",
       " 'mingkaman',\n",
       " 'gourmarharous',\n",
       " 'muntar',\n",
       " 'midseason',\n",
       " 'egg',\n",
       " 'josephine',\n",
       " 'axcp',\n",
       " 'kalimantan',\n",
       " 'bodyguard',\n",
       " 'matthewaffecte',\n",
       " 'cede',\n",
       " 'bankbase',\n",
       " 'rearreste',\n",
       " 'intersectorial',\n",
       " 'gajiram',\n",
       " 'injure',\n",
       " 'darfin',\n",
       " 'ion',\n",
       " 'nationally',\n",
       " 'turkey1',\n",
       " 'gambela',\n",
       " 'almeslimani',\n",
       " 'element',\n",
       " 'chuor',\n",
       " 'jeremie',\n",
       " 'sulaimani',\n",
       " 'pradesh',\n",
       " 'natural',\n",
       " 'gazel',\n",
       " 'landlordism',\n",
       " 'sena',\n",
       " 'franco',\n",
       " 'malaysia',\n",
       " 'naldanga',\n",
       " 'endofyear',\n",
       " 'mpimba',\n",
       " 'starvation',\n",
       " 'revolucionaria',\n",
       " 'cfrabidjan',\n",
       " 'barawa',\n",
       " 'overstretch',\n",
       " 'guit',\n",
       " 'saudiuaele',\n",
       " 'hadba',\n",
       " 'champaran',\n",
       " 'qaedalinke',\n",
       " 'heed',\n",
       " 'nhp',\n",
       " 'chloride',\n",
       " 'joinery',\n",
       " 'threshold',\n",
       " 'phq',\n",
       " 'emergencyipc',\n",
       " 'kutum',\n",
       " 'innee',\n",
       " 'numerous',\n",
       " 'backyard',\n",
       " 'turrialba',\n",
       " 'hmaid',\n",
       " 'bria',\n",
       " 'reportedcase',\n",
       " 'weeksa',\n",
       " 'toohigh',\n",
       " 'serum',\n",
       " 'capitalize',\n",
       " 'italyspecific',\n",
       " 'underreported',\n",
       " 'kilocalorie',\n",
       " 'zimvac',\n",
       " 'gida',\n",
       " 'nuevo',\n",
       " 'reportedmalaria',\n",
       " 'democracy',\n",
       " 'aegypti',\n",
       " 'spillage',\n",
       " 'arturo',\n",
       " 'junta',\n",
       " 'residual',\n",
       " 'currently',\n",
       " 'guinean',\n",
       " 'kg',\n",
       " 'cu',\n",
       " ...}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem_tokens_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'a food crisis is looming in the country with this season’s maize harvest expected to be 20 per cent less than the projected 40 million bags due to erratic rainfall coupled with fall armyworm invasion in the grain baskets the drought experienced in parts of north rift mid this year resulted in more than 40000 acres of maize crop in uasin gishu county alone withering this translates to a total loss of about 800000 bags of maize normally an acre produces at least 20 bags uasin gishu county executive in charge of agriculture dr cyril cheruiyot said most of the affected crops were those in tasselling stages of growth “the last time a drought of that magnitude was experienced in this region was in 1984 at least we have started receiving rain meaning the entire crop is not affected” he said',\n",
       " 'label': 0,\n",
       " 'vector': array([-0.07098591, -0.06682734,  0.10348069, -0.07041889, -0.02647143,\n",
       "        -0.23205143,  0.14404863, -0.025588  ,  0.2747087 , -0.06441002,\n",
       "         0.05091041, -0.17173819,  0.23475371, -0.08807612, -0.2239609 ,\n",
       "        -0.04399858, -0.16786893, -0.19758874, -0.549309  ,  0.05020534,\n",
       "         0.27880552,  0.22396177, -0.05975653,  0.23614648,  0.14423032,\n",
       "         0.208434  ,  0.25803205, -0.10425597, -0.27804127,  0.05623136,\n",
       "        -0.04282761,  0.15454513, -0.2805061 , -0.15621072, -0.43083277,\n",
       "         0.22199766,  0.07831463,  0.12396235,  0.02043026, -0.16986623,\n",
       "         0.03647088, -0.05571232, -0.02401086, -0.07895709,  0.13258013,\n",
       "         0.06198774,  0.28034532, -0.14802213, -0.12721561,  0.0281849 ,\n",
       "         0.2287249 , -0.01232166, -0.21108021,  0.22702473, -0.09576763,\n",
       "         0.04724063,  0.20772451,  0.02672507, -0.0534866 , -0.41067153,\n",
       "        -0.21003963, -0.45671538,  0.07720651,  0.24385603, -0.08926709,\n",
       "        -0.11798628,  0.35635293, -0.16886348,  0.05570018,  0.01259095,\n",
       "         0.28082785,  0.39518204, -0.22103342,  0.10411862, -0.52059704,\n",
       "         0.14872259,  0.03443957,  0.13198644, -0.02559025,  0.23124984,\n",
       "        -0.10645391, -0.3378914 ,  0.2093837 ,  0.01203882,  0.2505983 ,\n",
       "         0.18336207,  0.21109432, -0.19720322,  0.21877693,  0.08879606,\n",
       "        -0.11529747, -0.2646985 , -0.11317751,  0.04237208,  0.24995905,\n",
       "        -0.1226584 ], dtype=float32),\n",
       " 'tokens': [season,\n",
       "  20 per cent,\n",
       "  40 million,\n",
       "  mid this year,\n",
       "  more than 40000 acres,\n",
       "  uasin gishu county,\n",
       "  at least 20,\n",
       "  uasin gishu county,\n",
       "  1984]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs_nlp = {}\n",
    "for id in train_docs.keys():\n",
    "    doc = lem(train_docs[id][\"text\"])\n",
    "    train_docs[id][\"vector\"] = doc.vector\n",
    "    train_docs[id][\"tokens\"] = list(doc.ents)\n",
    "    \n",
    "    for token in \n",
    "    pattern=r''\n",
    "\n",
    "train_docs[9655]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate and report the sparsity rate of the vectors of train, validation, and test sets,\n",
    "#namely what percentages of the vectors in each set are filled with zeros.\n",
    "\n",
    "\n",
    "for sentence in train_docs.items():\n",
    "    weighted = sentence[\"vector\"]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a check how many numbers we have altogether in the texts\n",
    "pattern = r'-?\\d+'\n",
    "\n",
    "# Extract numbers using regular expression and create a set of numbers\n",
    "contained_numbers = [int(match) for string in lem_tokens_spacy for match in re.findall(pattern, string)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contained_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 21,\n",
       " 22,\n",
       " 22,\n",
       " 23,\n",
       " 23,\n",
       " 23,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 24,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 28,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 31,\n",
       " 32,\n",
       " 32,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 35,\n",
       " 35,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 36,\n",
       " 37,\n",
       " 37,\n",
       " 38,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 44,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 47,\n",
       " 47,\n",
       " 49,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 50,\n",
       " 51,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 53,\n",
       " 53,\n",
       " 54,\n",
       " 54,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 66,\n",
       " 67,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 77,\n",
       " 77,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 83,\n",
       " 85,\n",
       " 85,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 90,\n",
       " 93,\n",
       " 100,\n",
       " 100,\n",
       " 100,\n",
       " 102,\n",
       " 106,\n",
       " 107,\n",
       " 109,\n",
       " 115,\n",
       " 116,\n",
       " 119,\n",
       " 120,\n",
       " 120,\n",
       " 120,\n",
       " 124,\n",
       " 130,\n",
       " 130,\n",
       " 131,\n",
       " 134,\n",
       " 135,\n",
       " 139,\n",
       " 139,\n",
       " 150,\n",
       " 151,\n",
       " 157,\n",
       " 160,\n",
       " 160,\n",
       " 161,\n",
       " 170,\n",
       " 173,\n",
       " 174,\n",
       " 185,\n",
       " 190,\n",
       " 195,\n",
       " 200,\n",
       " 200,\n",
       " 209,\n",
       " 225,\n",
       " 227,\n",
       " 250,\n",
       " 250,\n",
       " 251,\n",
       " 270,\n",
       " 287,\n",
       " 300,\n",
       " 300,\n",
       " 300,\n",
       " 310,\n",
       " 330,\n",
       " 338,\n",
       " 350,\n",
       " 350,\n",
       " 356,\n",
       " 360,\n",
       " 378,\n",
       " 399,\n",
       " 415,\n",
       " 420,\n",
       " 426,\n",
       " 433,\n",
       " 434,\n",
       " 451,\n",
       " 466,\n",
       " 475,\n",
       " 500,\n",
       " 500,\n",
       " 558,\n",
       " 569,\n",
       " 584,\n",
       " 600,\n",
       " 648,\n",
       " 650,\n",
       " 650,\n",
       " 700,\n",
       " 750,\n",
       " 800,\n",
       " 846,\n",
       " 849,\n",
       " 850,\n",
       " 952,\n",
       " 1000,\n",
       " 1048,\n",
       " 1150,\n",
       " 1155,\n",
       " 1339,\n",
       " 1500,\n",
       " 1500,\n",
       " 1620,\n",
       " 1629,\n",
       " 1666,\n",
       " 1980,\n",
       " 1990,\n",
       " 2000,\n",
       " 2007,\n",
       " 2010,\n",
       " 2015,\n",
       " 2015,\n",
       " 2015,\n",
       " 2015,\n",
       " 2016,\n",
       " 2016,\n",
       " 2016,\n",
       " 2016,\n",
       " 2017,\n",
       " 2017,\n",
       " 2017,\n",
       " 2017,\n",
       " 2017,\n",
       " 2018,\n",
       " 2479,\n",
       " 2500,\n",
       " 2645,\n",
       " 2708,\n",
       " 2825,\n",
       " 3000,\n",
       " 3200,\n",
       " 3240,\n",
       " 3300,\n",
       " 3400,\n",
       " 3500,\n",
       " 4000,\n",
       " 4200,\n",
       " 4400,\n",
       " 4550,\n",
       " 4735,\n",
       " 5000,\n",
       " 5200,\n",
       " 5400,\n",
       " 5545,\n",
       " 6100,\n",
       " 6167,\n",
       " 6300,\n",
       " 6494,\n",
       " 6500,\n",
       " 6500,\n",
       " 6750,\n",
       " 7000,\n",
       " 7000,\n",
       " 7000,\n",
       " 7091,\n",
       " 8000,\n",
       " 8015,\n",
       " 9667,\n",
       " 10000,\n",
       " 10000,\n",
       " 10000,\n",
       " 12000,\n",
       " 12017,\n",
       " 12018,\n",
       " 12978,\n",
       " 13712,\n",
       " 15400,\n",
       " 19000,\n",
       " 25000,\n",
       " 25000,\n",
       " 26967,\n",
       " 30035,\n",
       " 30600,\n",
       " 33875,\n",
       " 42016,\n",
       " 49112,\n",
       " 50000,\n",
       " 50000,\n",
       " 57050,\n",
       " 62600,\n",
       " 71932,\n",
       " 72361,\n",
       " 83000,\n",
       " 98709,\n",
       " 100000,\n",
       " 107735,\n",
       " 141636,\n",
       " 147936,\n",
       " 160000,\n",
       " 160464,\n",
       " 185867,\n",
       " 195297,\n",
       " 201707,\n",
       " 231971,\n",
       " 242017,\n",
       " 378000,\n",
       " 400000,\n",
       " 780000,\n",
       " 1782850,\n",
       " 1817842,\n",
       " 2017508,\n",
       " 2018165,\n",
       " 2018174,\n",
       " 20152016,\n",
       " 54240000]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(contained_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_tokens_spacy.add(\"<oov>\")\n",
    "lem_tokens_spacy.add(\"<num>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      5\u001b[0m count_vect \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[1;32m----> 6\u001b[0m X_train_counts \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_bunch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m X_train_counts\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32m~\\PycharmProjects\\mlpcig\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1380\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m             )\n\u001b[0;32m   1386\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\PycharmProjects\\mlpcig\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1274\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1276\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\PycharmProjects\\mlpcig\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\PycharmProjects\\mlpcig\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "#ignore for now\n",
    "#this is the try with some standard sklearn tokenizer. this one doesn't won't yet bcs it's meant for documents\n",
    "# so if we want to work with it, the text needs to be split in some other way\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train_bunch.items())\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-training\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Training and Results Analysis (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "\n",
    "To evaluate the models, use <ins>accuracy</ins> as the metric throughout the task. \n",
    "\n",
    "**Dummy baseline (2 points).** Create one dummy baseline classifier that predicts the validation/test labels only based on the distribution of the labels in the training set (without any use of the feature vectors). This is a weak baseline and acts as a sanity check for the actual classifiers.\n",
    "\n",
    "**Training and tuning classifiers (5 points).** Select at least <ins>two classification algorithms</ins> from standard machine learning classifiers. Using each classification algorithm, train a machine learning model on each of the variations of feature vectors. This should result in <ins>four experiment sets</ins> (2 variations of feature vectors × 2 classification algorithms). The ML model in each of the experiments possibly have several involving hyper-parameters. For each experiment, select <ins>one of the hyper-parameters and tune its value</ins>. The tuning process is done by first assigning at least <ins>three values</ins> to the hyper-parameter, then training separate models based on each value, and finally using the evaluation results on the validation set to select the best-performing model. Report the studied hyper-parameters, the evaluation results of each on validation set, and finally the selected value of the hyper-parameter. \n",
    "\n",
    "**Evaluation, reporting results, and discussion (3 point).** Evaluate the selected models of the four experiments on the test set. Report the results of <ins>the four experiments on both validation and test sets (side by side) in one table as well as in one plot</ins>. Compare different experiments and models. Are the test results lower(/higher) than the validation results? If it is the case, where can it be rooted from? Among all these models and variations, what are the most important factors improving the classification results?\n",
    "\n",
    "**Confusion matrix (2 point).** Select the best performing model among the experiments and use it to create a confusion matrix. The matrix shows the predicted versus true results per each label. Explain your observations on the matrix. Across which classes do you observe significant confusions?\n",
    "\n",
    "**Features visualization (3 point).** Continue with the best performing model and now take its feature vectors for the *dataitems in the test set*. Project these feature vectors to a 2-dimensional space using the TSNE method.  Using these 2-dimensional vectors, create two plots where the dataitems are shown as points (small circles) on the plots. The plots look exactly the same but only differ in the coloring of the data points. The first plot colors every dataitem with its *true label*, while the second one colors each according to its *predicted label by the model*. Keep in mind to assign the same colors to the classes of the plots, so that the plots are visually comparable. Put these two plots side by side, observe the differences, and compare the results. Report your observations.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-optional\"></a><h2 style=\"color:rgb(0,120,170)\">Task C: Linear Model Interpretability (2 extra points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:rgb(224, 243, 255)\">\n",
    "Train a logistic regression model on one of the document representations. Take the coefficient weights, learned by the model, on each dimension (which here corresponds to each token in the dictionary). Separately for each class, study what are the tokens that have the highest contributions/importance for the predictions of the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
