{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">344.075 KV: Natural Language Processing (WS2022/23)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 3</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Document Classification with PyTorch and BERT</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University (JKU) Linz, and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Authors:** Navid Rekab-saz, Oleg Lesota<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-tensorboard\"><li style=\"font-size:large;font-weight:bold\">Bonus Task: Logging and Publishing Experiment Results (2 extra point)</li></a>\n",
    "    <a href=\"#section-taskA\"><li style=\"font-size:large;font-weight:bold\">Task A: Document Classification with PyTorch (25 points)</li></a>\n",
    "    <a href=\"#section-taskB\"><li style=\"font-size:large;font-weight:bold\">Task B: Document Classification with BERT (15 points)</li></a>\n",
    "    \n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment objective\n",
    "This assignment aims to provide the necessary practices for learning the principles of deep learning programing in NLP using PyTorch. To this end, Task A provides the space for becoming fully familiar with PyTorch programming by implementing a \"simple\" document (sentence) classification model with PyTorch, and Task B extends this classifier with a BERT model. As the assignment requires working with PyTorch and Huggingface Transformers, please familiarize yourself with these libraries using any possible available teaching resources in particular the libraries' documentations. The assignment has in total **40 points**, and also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Dataset\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python`, `PyTorch` and, `transformers`. Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "To conduct the experiments, we use a subset of the `HumSet` dataset [1] (https://blog.thedeep.io/humset/). `HumSet` is created by the DEEP (https://www.thedeep.io) project – an open source platform which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset contains the classes (labels) referring to the humanitarian sectors like agriculture, health, and protection. The dataset contains an overall number of 17,301 data points. \n",
    "\n",
    "Download the dataset from the Moodle page of the course.\n",
    "\n",
    "the provided zip file consists of the following files:\n",
    "- `thedeep.subset.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.label.txt`: Captions of the labels.\n",
    "- `thedeep.ToU.txt`: Terms of use of the dataset.\n",
    "\n",
    "[1] HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response\n",
    "*Selim Fekih, Nicolo' Tamagnone, Benjamin Minixhofer, Ranjan Shrestha, Ximena Contla, Ewan Oglethorpe and Navid Rekabsaz.* \n",
    "In Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP), December 2022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission\n",
    "\n",
    "Each group should submit the following two files:\n",
    "\n",
    "- One Jupyter Notebook file (`.ipynb`), containing all the code, results, visualizations, etc. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that (if necessary) one can run all the cells from top to bottom without any error. Do not forget to put in your names and student numbers in the first cell of the Notebook. \n",
    "- The HTML file (`.html`) achieved from exporting the Jupyter Notebook to HTML (Download As HTML).\n",
    "\n",
    "You do not need to include the data files in the submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-tensorboard\"></a><h2 style=\"color:rgb(0,120,170)\">Bonus Task: Logging and Publishing Experiment Results (2 extra point)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all experiments of this assignment, use any experiment monitoring tool like [`TensorBoard`](https://www.tensorflow.org/tensorboard), [`wandb`](https://wandb.ai) to log and store all useful information about the training and evaluation of the models. Feel free to log any important aspect in particular the changes in evaluation results on validation, in training loss, and in learning rate.\n",
    "\n",
    "After finalizing all experiments and cleaning any unnecessary experiment, **provide the URL to the results monitoring page below**.\n",
    "\n",
    "For instance if using [`TensorBoard.dev`](https://tensorboard.dev), you can run the following command in the folder of log files: `tensorboard dev upload --name my_exp --logdir path/to/output_dir`, and take the provided URL to the TensorBoard's console.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL :** *EDIT!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskA\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Document Classification with PyTorch (25 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this task is identical to the one of Assignment 2 - Task B, namely to design a document classification model that exploits pre-trained word embeddings. It is of course allowed to use the preprocessed text, the dictionary, or any other relevant code or processings, done in the previous assignments.\n",
    "\n",
    "In this task, you implement a document classification model using PyTorch, which given a document/sentence (consisting of a set of words) predicts the corresponding class. Before getting started with coding, have a look at the <a href=\"#section-tensorboard\">optional task</a>, as you may want to already include `Tensorboard` in the code. The implementation of the classifier should cover the points below.\n",
    "\n",
    "**Preprocessing and dictionary (1 point):** Following previous assignments, load the train, validation, and test datasets, apply necessary preprocessing steps, and create a dictionary of words. \n",
    "\n",
    "**Data batching (4 points):** Using the dictionary, create batches for any given dataset (train/validation/test). Each batch is a two-dimensional matrix of *batch-size* to *max-document-length*, containing the IDs of the words in the corresponding documents. *Batch-size* and *max-document-length* are two hyper-parameters and can be set to any appropriate values (*Batch-size* must be higher than 1 and *max-document-length* at least 50 words). If a document has more than *max-document-length* words, only the first *max-document-length* words should be kept.\n",
    "\n",
    "**Word embedding lookup (2 point):** Using `torch.nn.Embedding`, create a lookup for the embeddings of all the words in the dictionary. The lookup is in fact a matrix, which maps the ID of each word to the corresponding word vector. Similar to Assignment 2, use the pre-trained vectors of a word embedding model (like word2vec or GloVe) to initialize the word embeddings of the lookup. Keep in mind that the embeddings of the words in the lookup should be matched with the correct vector in the pretrained word embedding. If the vector of a word in the lookup does not exist in the pretrained word embeddings, the corresponding vector should be initialized randomly. \n",
    "\n",
    "**Model definition (3 points):** Define the class `ClassificationAverageModel` as a PyTorch model. In the initialization procedure, the model receives the word embedding lookup, and includes it in the model as model's parameters. These embeddings parameters should be trainable, meaning that the word vectors get updated during model training. Feel free to add any other parameters to the model, which might be necessary for accomplishing the functionalities explained in the following.\n",
    "\n",
    "**Forward function (5 points):** The forward function of the model receives a batch of data, and first fetches the corresponding embeddings of the word IDs in the batch using the lookup. Similar to Assignment 2, the embedding of a document is created by calculating the *element-wise mean* of the embeddings of the document's words. Formally, given the document $d$, consisting of words $\\left[ v_1, v_2, ..., v_{|d|} \\right]$, the document representation $\\mathbf{e}_d$ is defined as:\n",
    "\n",
    "<center><div>$\\mathbf{e}_d = \\frac{1}{|d|}\\sum_{i=1}^{|d|}{\\mathbf{e}_{v_i}}$</div></center>\n",
    "\n",
    "where $\\mathbf{e}_{v}$ is the vector of the word $v$, and $|d|$ is the length of the document. An important point in the implementation of this formula is that the documents in the batch might have different lengths and therefore each document should be divided by its corresponding $|d|$. Finally, this document embedding is utilized to predict the probability of the output classes, done by applying a linear transformation from the embeddings size to the number of classes, followed by Softmax. The linear transformation also belongs to the model's parameters and will be learned in training.\n",
    "\n",
    "**Loss Function and optimization (2 point):** The loss between the predicted and the actual classes is calculated using Negative Log Likelihood or Cross Entropy. Update the model's parameters using any appropriate optimization mechanism such as Adam.\n",
    "\n",
    "**Early Stopping (2 points):** After each epoch, evaluate the model on the *validation set* using accuracy. If the evaluation result (accuracy) improves, save the model as the best performing one so far. If the results are not improving after a certain number of evaluation rounds (set as another hyper-parameter) or if training reaches a certain number of epochs, terminate the training procedure. \n",
    "\n",
    "**Test Set Evaluation (1 point):** After finishing the training, load the (already stored) best performing model, and use it for class prediction on the test set.\n",
    "\n",
    "**Reporting (1 point):** During loading and processing the collection, provide sufficient information and examples about the data and the applied processing steps. Report the results of the best performing model on the validation and test set in a table.\n",
    "\n",
    "**Overall functionality of the training procedure (4 point).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and dictionary (1 point): \n",
    "# Following previous assignments, load the train, validation, and test datasets, \n",
    "# apply necessary preprocessing steps, and create a dictionary of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import random\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# function to split the data into text and labels\n",
    "def split_into_texts_and_labels(data):\n",
    "    labels = []\n",
    "    texts = data.split('\\n')\n",
    "    texts = [text for text in texts if len(text)>0]\n",
    "    for i, text in enumerate(texts):\n",
    "        try:\n",
    "            nr = int(text[-1])\n",
    "            labels.append(text[-1])\n",
    "            texts[i] = ''.join(list(text)[:-2])\n",
    "        except:\n",
    "            break\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# apply some basic preprocessing methods\n",
    "# such as replacing all the dates and numbers with a special token \n",
    "# and removing special characters\n",
    "def perform_basic_preprocessing(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # replace the dates with <date>\n",
    "    for d in re.finditer('\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', text):\n",
    "        text = text.replace(str(d.group()), '<date>')\n",
    "\n",
    "    for d in re.finditer(r'(\\d{1,2}[\\s]?|\\d{1,2}[\\s]?th[\\s]?)?(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[\\s-]?(\\d{1,2})?[,\\s-]?[\\s]?\\d{4}', text, re.I|re.M):\n",
    "        text = text.replace(str(d.group()), '<date>')\n",
    "\n",
    "    for d in re.finditer(r'(\\d{1,2}|\\d{1,2}[\\s]?th)[\\s]?(?:january|february|march|april|may|june|july|august|september|october|november|december|jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)', text, re.I|re.M):  \n",
    "        text = text.replace(str(d.group()), '<date>')\n",
    "\n",
    "    for d in re.finditer(r'\\d{1,2}[/-]\\d{4}', text, re.M|re.I):\n",
    "        text = text.replace(str(d.group()), '<date>')\n",
    "        \n",
    "    # replace the remaining numbers with <num>\n",
    "    text = re.sub('[0-9]*[[,.]?[0-9]+]*', '<num>', text)\n",
    "    \n",
    "    # remove all special characters besides <>\n",
    "    text = re.sub('([^a-z<>])+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# split the text into single tokens\n",
    "def perform_tokenization(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # define tokenizer for special tokens\n",
    "    tokenizer = MWETokenizer([('<', 'num', '>'), ('<', 'date', '>')])\n",
    "    tokens = tokenizer.tokenize(nltk.word_tokenize(text))\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens = [token for token in tokens if not token in stop_words]\n",
    "    # perform lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_256214/4152795208.py:38: FutureWarning: Possible nested set at position 7\n",
      "  text = re.sub('[0-9]*[[,.]?[0-9]+]*', '<num>', text)\n"
     ]
    }
   ],
   "source": [
    "# Load the train, validation, and test sets. \n",
    "files = glob.glob(os.path.join('nlp2023_24_data', '**', '*'))\n",
    "\n",
    "for file in files:\n",
    "    if os.path.basename(file).__contains__('train'):\n",
    "        with open(file, 'r') as fh:\n",
    "            train_data = fh.read()\n",
    "    elif os.path.basename(file).__contains__('test'):\n",
    "        with open(file, 'r') as fh:\n",
    "            test_data = fh.read()\n",
    "    elif os.path.basename(file).__contains__('validation'):\n",
    "        with open(file, 'r') as fh:\n",
    "            val_data = fh.read()\n",
    "    elif os.path.basename(file).__contains__('labels'):\n",
    "        with open(file, 'r') as fh:\n",
    "            labels = fh.read()\n",
    "\n",
    "# split each data file into a list of texts and a list of their corresponding labels\n",
    "train_texts, train_labels = split_into_texts_and_labels(train_data)\n",
    "val_texts, val_labels = split_into_texts_and_labels(val_data)\n",
    "test_texts, test_labels = split_into_texts_and_labels(test_data)\n",
    "# print(len(train_texts))\n",
    "\n",
    "# get a list of tokens (tokens_per_text) for each preprocessed text in the training data \n",
    "tokens_per_text_train = []\n",
    "for text in train_texts:\n",
    "    # print(text)\n",
    "    tokens_per_text_train.append(perform_tokenization(perform_basic_preprocessing(text)))\n",
    "\n",
    "\n",
    "# generate one dictionary containing all tokens that appear accross all texts \n",
    "# and count the number of their overall occurences\n",
    "word_dict = dict(Counter(list(itertools.chain(*tokens_per_text_train))))\n",
    "# sort the dictionary by the number of word occurences\n",
    "word_dict = dict(sorted(word_dict.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "threshold = 100 \n",
    "smaller_word_dict = {k:v for (k,v) in word_dict.items() if v > threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12110\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data batching (4 points): \n",
    "Using the dictionary, create batches for any given dataset (train/validation/test). \n",
    "Each batch is a two-dimensional matrix of batch-size to max-document-length, \n",
    "containing the IDs of the words in the corresponding documents. \n",
    "Batch-size and max-document-length are two hyper-parameters and can be set to any appropriate values \n",
    "(Batch-size must be higher than 1 and max-document-length at least 50 words). \n",
    "If a document has more than max-document-length words, only the first max-document-length words should be kept.\n",
    "'''\n",
    "\n",
    "# two hyperparams\n",
    "bs = 16 # 32, 64...\n",
    "max_doc_len = 80 \n",
    "\n",
    "# smaller_word_dict => key is word, value is occurence\n",
    "# can add OOV to smaller_word_dict\n",
    "smaller_word_dict['oov'] = 0\n",
    "\n",
    "def get_batch(bs, max_doc_len, dataset, word_dict):\n",
    "    \n",
    "    batch = np.zeros((bs, max_doc_len))\n",
    "    current_pos = 0\n",
    "    mult_factor = 0\n",
    "    \n",
    "    for i, data_point in enumerate(dataset):\n",
    "        if i+bs >= len(dataset):\n",
    "            break\n",
    "        if i % bs == 0 and i > 0:\n",
    "            current_pos += bs\n",
    "            yield torch.IntTensor(batch), current_pos\n",
    "            mult_factor += 1\n",
    "        \n",
    "        counter = itertools.count(0)\n",
    "        # discard words that are not in the word_dict\n",
    "        doc_word_ids = [list(word_dict).index(word) for word in (perform_tokenization(perform_basic_preprocessing(data_point))) if word in word_dict.keys() and next(counter) < max_doc_len]\n",
    "        \n",
    "        if len(doc_word_ids) < max_doc_len:\n",
    "            # print(np.array(doc_word_ids).shape)\n",
    "            # print(np.full((max_doc_len-len(doc_word_ids)), list(word_dict).index('oov')).shape)\n",
    "            batch[i-(bs*mult_factor)] = np.concatenate((np.array(doc_word_ids), np.full((max_doc_len-len(doc_word_ids)), list(word_dict).index('oov'))), axis=0)\n",
    "        else:\n",
    "            batch[i-(bs*mult_factor)] = np.array(doc_word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Word embedding lookup (2 point): Using torch.nn.Embedding, create a lookup for the embeddings of all the words \n",
    "in the dictionary. The lookup is in fact a matrix, which maps the ID of each word to the corresponding word vector. \n",
    "Similar to Assignment 2, use the pre-trained vectors of a word embedding model (like word2vec or GloVe)\n",
    "to initialize the word embeddings of the lookup. Keep in mind that the embeddings of the words in the lookup \n",
    "should be matched with the correct vector in the pretrained word embedding. If the vector of a word in the lookup \n",
    "does not exist in the pretrained word embeddings, the corresponding vector should be initialized randomly.\n",
    "'''\n",
    "\n",
    "embeddings_dict = {}\n",
    "with open('glove.6B.300d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], 'float32')\n",
    "        embeddings_dict[word] = vector\n",
    "# print(embeddings_dict['dog']) \n",
    "\n",
    "# can also create a new dictionary where we map each word to an index\n",
    "# word2idx = {w:i for i,w in enumerate(smaller_word_dict.keys())}\n",
    "        \n",
    "embed_lookup = nn.Embedding(len(smaller_word_dict), 300)\n",
    "pretrained_weights = []\n",
    "for word in smaller_word_dict.keys():\n",
    "    try:\n",
    "        pretrained_weights.append(embeddings_dict[word])\n",
    "    except:\n",
    "        pretrained_weights.append(np.random.normal(0, 0.5, 300))\n",
    "\n",
    "embed_lookup.weight.data.copy_(torch.from_numpy(np.array(pretrained_weights)))\n",
    "\n",
    "ids = [list(smaller_word_dict).index(word) for word in smaller_word_dict.keys()]\n",
    "word_vecs = embed_lookup(torch.LongTensor(ids)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.random.normal(0, 1, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word_vecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1740e-02, -1.1853e-01, -5.0232e-02, -6.1991e-02, -9.4787e-02,\n",
      "         -2.1333e-01, -7.5989e-02,  1.4220e-01, -4.0274e-02, -9.1852e-02,\n",
      "         -2.3542e-02,  2.0936e-02],\n",
      "        [ 4.2973e-02, -5.5664e-02,  3.2067e-03, -6.0115e-02,  2.7030e-02,\n",
      "         -6.0373e-03, -2.7182e-02,  5.5100e-02, -5.5786e-02, -3.3384e-02,\n",
      "          1.5456e-02, -4.7657e-03],\n",
      "        [ 2.4145e-02, -6.7838e-02, -1.1615e-02, -5.6722e-02, -4.0764e-03,\n",
      "         -4.1419e-02, -3.0782e-02,  6.5439e-02, -5.2047e-02, -4.0955e-02,\n",
      "          1.2831e-02,  4.5219e-03],\n",
      "        [ 2.5934e-02, -6.4447e-02, -1.1385e-02, -5.6997e-02, -7.7177e-03,\n",
      "         -4.5333e-02, -3.6238e-02,  6.7079e-02, -5.2839e-02, -4.3493e-02,\n",
      "          1.2493e-02, -6.8279e-04],\n",
      "        [ 4.4054e-02, -5.4645e-02,  1.0365e-02, -5.9963e-02,  3.1558e-02,\n",
      "          3.8349e-03, -1.9458e-02,  4.7000e-02, -5.0976e-02, -2.3565e-02,\n",
      "          2.4668e-02, -1.3767e-03],\n",
      "        [ 1.0293e-02, -7.8010e-02, -1.4640e-02, -5.3300e-02, -1.5331e-02,\n",
      "         -6.8881e-02, -3.9144e-02,  7.5853e-02, -4.8674e-02, -4.2718e-02,\n",
      "          1.0711e-02,  9.0619e-03],\n",
      "        [ 3.9727e-02, -6.7142e-02,  6.7689e-03, -6.1998e-02,  2.4006e-02,\n",
      "         -1.5732e-02, -2.7099e-02,  5.2130e-02, -5.4694e-02, -3.6486e-02,\n",
      "          1.8959e-02, -2.3399e-03],\n",
      "        [ 2.8289e-02, -6.7185e-02, -6.4474e-03, -5.8811e-02,  5.3165e-03,\n",
      "         -3.2670e-02, -3.1650e-02,  6.2900e-02, -5.5113e-02, -3.9761e-02,\n",
      "          1.0170e-02,  3.9311e-04],\n",
      "        [ 3.9197e-02, -6.6086e-02,  9.1632e-04, -5.6643e-02,  2.6103e-02,\n",
      "         -5.9968e-03, -2.7919e-02,  5.0075e-02, -6.2073e-02, -3.2655e-02,\n",
      "          1.6071e-02,  5.1172e-03],\n",
      "        [-4.5554e-02, -1.3998e-01, -7.9366e-02, -5.5604e-02, -1.4352e-01,\n",
      "         -2.8561e-01, -1.0506e-01,  1.8006e-01, -3.4451e-02, -1.1352e-01,\n",
      "         -3.7418e-02,  2.7304e-02],\n",
      "        [ 2.1184e-02, -7.2724e-02, -1.7950e-02, -5.8683e-02, -1.4529e-02,\n",
      "         -6.0457e-02, -4.1350e-02,  7.4921e-02, -4.8349e-02, -4.6415e-02,\n",
      "          4.5305e-03,  7.4443e-03],\n",
      "        [ 2.2649e-02, -7.4332e-02, -1.2371e-02, -5.6014e-02, -7.6492e-03,\n",
      "         -5.8347e-02, -3.8894e-02,  7.2947e-02, -5.3787e-02, -5.2258e-02,\n",
      "          4.8270e-03,  1.4939e-04],\n",
      "        [ 5.0303e-02, -5.8646e-02,  1.1972e-02, -5.5676e-02,  3.7259e-02,\n",
      "          9.9765e-03, -1.8614e-02,  4.4375e-02, -5.4863e-02, -2.6230e-02,\n",
      "          2.1208e-02,  1.6510e-03],\n",
      "        [ 3.1995e-02, -6.3654e-02,  1.2925e-04, -5.5837e-02,  2.2420e-02,\n",
      "         -1.9753e-02, -2.8084e-02,  5.5224e-02, -5.0296e-02, -3.0324e-02,\n",
      "          1.6457e-02, -2.0102e-04],\n",
      "        [-5.1024e-02, -1.3958e-01, -8.1703e-02, -5.3358e-02, -1.6201e-01,\n",
      "         -2.9717e-01, -1.0158e-01,  1.7669e-01, -4.0666e-02, -1.2230e-01,\n",
      "         -3.4893e-02,  2.3903e-02],\n",
      "        [-3.0260e-02, -1.1379e-01, -6.6159e-02, -5.2213e-02, -1.0712e-01,\n",
      "         -2.1443e-01, -7.8370e-02,  1.4860e-01, -5.4251e-02, -8.8146e-02,\n",
      "         -2.4232e-02,  2.2117e-02]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model definition (3 points): Define the class ClassificationAverageModel as a PyTorch model. \n",
    "In the initialization procedure, the model receives the word embedding lookup, \n",
    "and includes it in the model as model's parameters. These embeddings parameters should be trainable, \n",
    "meaning that the word vectors get updated during model training. Feel free to add any other parameters to the model, \n",
    "which might be necessary for accomplishing the functionalities explained in the following.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Forward function (5 points): The forward function of the model receives a batch of data, \n",
    "and first fetches the corresponding embeddings of the word IDs in the batch using the lookup. \n",
    "Similar to Assignment 2, the embedding of a document is created by calculating the element-wise mean of \n",
    "the embeddings of the document's words. Formally, given the document 𝑑, consisting of words [𝑣1,𝑣2,...,𝑣|𝑑|], \n",
    "the document representation 𝐞𝑑\n",
    "\n",
    "is defined as:\n",
    "𝐞𝑑=1|𝑑|∑|𝑑|𝑖=1𝐞𝑣𝑖\n",
    "\n",
    "where 𝐞𝑣\n",
    "is the vector of the word 𝑣, and |𝑑| is the length of the document. \n",
    "An important point in the implementation of this formula is that the documents in the batch might\n",
    "have different lengths and therefore each document should be divided by its corresponding |𝑑|. \n",
    "Finally, this document embedding is utilized to predict the probability of the output classes, \n",
    "done by applying a linear transformation from the embeddings size to the number of classes, followed by Softmax. \n",
    "The linear transformation also belongs to the model's parameters and will be learned in training.'''\n",
    "\n",
    "\n",
    "class ClassificationAverageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, lookup, docs, bs):\n",
    "        super(ClassificationAverageModel, self).__init__()\n",
    "        self.lookup = nn.Parameter(lookup)\n",
    "        self.lookup.requires_grad = True\n",
    "        \n",
    "        self.bs = bs\n",
    "        self.docs = docs\n",
    "        \n",
    "        self.linear = nn.Linear(300, 12)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch, current_pos):\n",
    "     \n",
    "        e_ds = []\n",
    "        for i, doc_ids in enumerate(batch):\n",
    "            doc_embeds = self.lookup[doc_ids.tolist()] \n",
    "            e_ds.append(np.sum(doc_embeds.tolist(), axis=0)/len(self.docs[current_pos-self.bs+i]))\n",
    "        \n",
    "        # return self.softmax(self.linear(torch.from_numpy(np.array(e_ds)).float()))\n",
    "        return self.linear(torch.from_numpy(np.array(e_ds)).float())\n",
    "    \n",
    "c = ClassificationAverageModel(word_vecs, train_texts, bs)\n",
    "for b, counter in get_batch(bs, max_doc_len, train_texts, smaller_word_dict):\n",
    "    print(c.forward(b, counter))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0234, grad_fn=<NllLossBackward0>)\n",
      "0.35209627329192544\n",
      "0.3684006211180124\n",
      "0.3815993788819876\n",
      "0.3951863354037267\n",
      "0.4041149068322981\n",
      "0.4079968944099379\n",
      "0.4091614906832298\n",
      "0.41459627329192544\n",
      "0.42080745341614906\n",
      "0.4231366459627329\n",
      "0.4254658385093168\n",
      "0.4309006211180124\n",
      "0.4359472049689441\n",
      "0.4394409937888199\n",
      "0.4452639751552795\n",
      "0.4495341614906832\n",
      "0.45846273291925466\n",
      "0.46195652173913043\n",
      "0.4658385093167702\n",
      "0.4720496894409938\n",
      "tensor(1.5143, grad_fn=<NllLossBackward0>)\n",
      "0.47787267080745344\n",
      "0.4813664596273292\n",
      "0.4871894409937888\n",
      "0.49145962732919257\n",
      "0.4937888198757764\n",
      "0.49767080745341613\n",
      "0.5007763975155279\n",
      "0.5046583850931677\n",
      "0.5108695652173914\n",
      "0.5124223602484472\n",
      "0.514751552795031\n",
      "0.5182453416149069\n",
      "0.5201863354037267\n",
      "0.5232919254658385\n",
      "0.5263975155279503\n",
      "0.529891304347826\n",
      "0.5349378881987578\n",
      "0.5357142857142857\n",
      "0.5372670807453416\n",
      "tensor(1.2599, grad_fn=<NllLossBackward0>)\n",
      "0.5392080745341615\n",
      "0.5415372670807453\n",
      "0.5438664596273292\n",
      "0.5442546583850931\n",
      "0.5461956521739131\n",
      "0.5485248447204969\n",
      "0.5489130434782609\n",
      "0.5508540372670807\n",
      "0.5539596273291926\n",
      "0.5551242236024845\n",
      "0.5562888198757764\n",
      "0.5570652173913043\n",
      "0.5593944099378882\n",
      "0.5609472049689441\n",
      "0.5613354037267081\n",
      "0.5617236024844721\n",
      "0.5632763975155279\n",
      "0.5636645962732919\n",
      "0.5652173913043478\n",
      "tensor(1.0913, grad_fn=<NllLossBackward0>)\n",
      "0.5659937888198758\n",
      "0.5663819875776398\n",
      "0.5679347826086957\n",
      "0.5698757763975155\n",
      "0.5706521739130435\n",
      "0.5714285714285714\n",
      "0.5725931677018633\n",
      "0.5733695652173914\n",
      "0.5741459627329193\n",
      "0.5745341614906833\n",
      "0.5756987577639752\n",
      "0.5768633540372671\n",
      "0.578027950310559\n",
      "0.5795807453416149\n",
      "0.5799689440993789\n",
      "0.5815217391304348\n",
      "0.5819099378881988\n",
      "0.5834627329192547\n",
      "tensor(0.9712, grad_fn=<NllLossBackward0>)\n",
      "0.5842391304347826\n",
      "0.5854037267080745\n",
      "0.5873447204968945\n",
      "0.5881211180124224\n",
      "0.5900621118012422\n",
      "0.5908385093167702\n",
      "0.5920031055900621\n",
      "0.592391304347826\n",
      "0.59277950310559\n",
      "0.593167701863354\n",
      "0.593944099378882\n",
      "0.5954968944099379\n",
      "0.5962732919254659\n",
      "0.5966614906832298\n",
      "0.5970496894409938\n",
      "0.5989906832298136\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loss Function and optimization (2 point): \n",
    "The loss between the predicted and the actual classes is calculated using Negative Log Likelihood or Cross Entropy.\n",
    "Update the model's parameters using any appropriate optimization mechanism such as Adam.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Early Stopping (2 points): After each epoch, evaluate the model on the validation set \n",
    "using accuracy. If the evaluation result (accuracy) improves, save the model as \n",
    "the best performing one so far. If the results are not improving after a certain \n",
    "number of evaluation rounds (set as another hyper-parameter) or if training reaches a \n",
    "certain number of epochs, terminate the training procedure.\n",
    "''' \n",
    "\n",
    "model = ClassificationAverageModel(word_vecs, train_texts, bs)\n",
    "loss_fct = nn.CrossEntropyLoss() # should already have softmax???, nn.NLLLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), 0.001)\n",
    "nr_epochs = 100\n",
    "count = 0\n",
    "old_acc = 0\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for batch, current_pos in get_batch(bs, max_doc_len, train_texts, smaller_word_dict):\n",
    "        optim.zero_grad()\n",
    "        out = model(batch, current_pos)\n",
    "        labels = train_labels[current_pos-bs:current_pos]\n",
    "        loss = loss_fct(out, torch.tensor(np.array(labels).astype('int')))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    # early stopping at the end of every epoch\n",
    "    acc = 0\n",
    "    nr_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch, val_current_pos in get_batch(bs, max_doc_len, val_texts, smaller_word_dict):\n",
    "            out = model(val_batch, val_current_pos)\n",
    "            # get max of out\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            acc += (torch.sum(pred == torch.tensor(np.array(val_labels[val_current_pos-bs:val_current_pos]).astype('int'))).item() / bs)\n",
    "            nr_batches += 1\n",
    "        acc /= nr_batches\n",
    "        # print(acc)\n",
    "        count +=1\n",
    "        if acc > old_acc:\n",
    "            old_acc = acc\n",
    "            print(acc)\n",
    "            # save_model\n",
    "            # torch.save(model.state_dict(), PATH)\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            count = 0\n",
    "        if count > 10:\n",
    "            # end training\n",
    "            # break  \n",
    "            print('Problem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 3, 9, 4, 1, 3, 3, 0, 4, 4, 3, 4, 9, 2, 4])\n",
      "['8', '9', '3', '9', '8', '1', '3', '3', '0', '1', '4', '3', '4', '9', '2', '4']\n",
      "tensor([4, 4, 1, 3, 4, 9, 4, 0, 3, 9, 9, 9, 4, 9, 4, 4])\n",
      "['4', '4', '1', '5', '4', '9', '4', '5', '3', '0', '9', '9', '4', '9', '4', '4']\n",
      "tensor([3, 9, 3, 9, 4, 8, 9, 9, 4, 9, 4, 9, 0, 3, 9, 4])\n",
      "['8', '2', '3', '4', '4', '8', '9', '9', '4', '9', '4', '9', '0', '3', '9', '4']\n",
      "tensor([2, 1, 3, 4, 3, 9, 0, 9, 3, 4, 9, 9, 4, 1, 0, 4])\n",
      "['2', '1', '3', '4', '5', '9', '0', '9', '1', '4', '9', '9', '8', '4', '0', '4']\n",
      "tensor([3, 1, 3, 8, 2, 4, 3, 9, 9, 9, 9, 4, 9, 4, 9, 4])\n",
      "['3', '7', '3', '8', '2', '4', '3', '9', '9', '1', '9', '4', '0', '4', '9', '4']\n",
      "tensor([9, 9, 3, 9, 0, 9, 9, 3, 9, 9, 3, 9, 9, 4, 9, 4])\n",
      "['9', '0', '3', '9', '9', '1', '9', '3', '2', '9', '3', '9', '9', '4', '5', '4']\n",
      "tensor([9, 4, 9, 4, 9, 0, 3, 0, 3, 4, 4, 2, 9, 1, 3, 3])\n",
      "['9', '4', '9', '4', '6', '0', '3', '0', '3', '4', '4', '2', '1', '1', '3', '3']\n",
      "tensor([4, 9, 3, 0, 4, 1, 0, 4, 9, 9, 3, 1, 3, 9, 8, 8])\n",
      "['4', '1', '3', '1', '4', '1', '6', '4', '6', '0', '3', '1', '3', '9', '8', '8']\n",
      "tensor([9, 0, 1, 1, 3, 9, 4, 3, 3, 3, 9, 3, 9, 8, 3, 1])\n",
      "['9', '5', '1', '0', '3', '3', '9', '3', '8', '3', '8', '3', '9', '8', '3', '1']\n",
      "tensor([3, 9, 0, 9, 9, 9, 3, 3, 9, 4, 9, 3, 9, 1, 3, 0])\n",
      "['3', '9', '5', '6', '1', '5', '3', '3', '4', '7', '9', '9', '9', '1', '3', '7']\n",
      "tensor([4, 4, 2, 9, 3, 4, 4, 9, 4, 9, 9, 4, 9, 9, 1, 4])\n",
      "['5', '3', '2', '4', '3', '4', '4', '9', '4', '9', '3', '4', '9', '0', '1', '4']\n",
      "tensor([1, 0, 9, 8, 9, 3, 2, 4, 2, 9, 9, 0, 4, 4, 9, 4])\n",
      "['5', '5', '9', '8', '4', '3', '2', '1', '2', '4', '9', '1', '4', '4', '9', '4']\n",
      "tensor([4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 0, 3, 1, 3, 4, 2])\n",
      "['4', '3', '4', '3', '4', '3', '4', '4', '4', '4', '0', '5', '1', '3', '4', '2']\n",
      "tensor([9, 9, 9, 9, 4, 9, 9, 9, 8, 4, 4, 1, 4, 3, 2, 4])\n",
      "['9', '9', '9', '9', '4', '9', '9', '9', '3', '4', '4', '9', '4', '3', '9', '4']\n",
      "tensor([9, 9, 1, 0, 9, 2, 2, 2, 9, 0, 0, 3, 3, 9, 4, 3])\n",
      "['9', '9', '1', '5', '4', '2', '1', '0', '9', '0', '0', '8', '5', '4', '4', '3']\n",
      "tensor([4, 4, 4, 3, 9, 0, 9, 4, 4, 3, 3, 4, 9, 9, 1, 3])\n",
      "['4', '7', '4', '3', '0', '0', '6', '4', '4', '3', '5', '4', '9', '9', '1', '3']\n",
      "tensor([9, 4, 4, 3, 9, 9, 9, 3, 9, 1, 4, 9, 3, 0, 9, 2])\n",
      "['5', '4', '4', '1', '0', '3', '9', '3', '9', '1', '4', '9', '5', '0', '9', '2']\n",
      "tensor([4, 4, 9, 9, 4, 0, 1, 4, 4, 1, 4, 9, 0, 3, 3, 4])\n",
      "['4', '4', '9', '1', '4', '4', '1', '4', '4', '1', '4', '0', '0', '9', '3', '4']\n",
      "tensor([8, 9, 4, 1, 9, 4, 9, 9, 3, 9, 9, 4, 3, 9, 3, 2])\n",
      "['8', '3', '6', '5', '9', '4', '5', '9', '2', '9', '9', '4', '3', '9', '3', '2']\n",
      "tensor([4, 3, 1, 3, 9, 9, 1, 4, 9, 4, 4, 1, 2, 3, 4, 9])\n",
      "['4', '3', '1', '3', '4', '9', '1', '5', '0', '4', '4', '4', '2', '8', '4', '9']\n",
      "tensor([2, 9, 9, 8, 4, 2, 4, 4, 0, 4, 4, 0, 3, 3, 4, 0])\n",
      "['2', '9', '1', '8', '4', '2', '4', '4', '5', '4', '4', '0', '3', '8', '4', '9']\n",
      "tensor([4, 3, 9, 3, 3, 9, 4, 0, 9, 9, 3, 0, 0, 9, 3, 2])\n",
      "['4', '3', '9', '3', '3', '2', '4', '1', '9', '9', '3', '0', '1', '9', '3', '2']\n",
      "tensor([3, 9, 9, 9, 4, 3, 4, 9, 9, 3, 9, 9, 9, 9, 9, 3])\n",
      "['3', '0', '9', '9', '4', '3', '4', '9', '5', '3', '4', '9', '9', '9', '1', '5']\n",
      "tensor([2, 9, 0, 4, 4, 4, 3, 9, 9, 9, 3, 9, 3, 9, 8, 4])\n",
      "['2', '2', '1', '4', '4', '1', '3', '9', '7', '0', '3', '2', '7', '9', '8', '4']\n",
      "tensor([4, 9, 3, 4, 9, 3, 0, 4, 4, 3, 4, 3, 9, 9, 3, 0])\n",
      "['4', '8', '0', '4', '0', '0', '0', '4', '4', '3', '4', '9', '9', '9', '3', '0']\n",
      "tensor([9, 2, 1, 3, 8, 4, 9, 9, 3, 4, 3, 3, 9, 9, 9, 9])\n",
      "['9', '2', '1', '6', '8', '4', '9', '4', '3', '4', '1', '3', '9', '5', '4', '6']\n",
      "tensor([3, 4, 4, 9, 1, 9, 0, 4, 9, 4, 9, 8, 3, 3, 9, 9])\n",
      "['3', '4', '4', '9', '1', '9', '1', '4', '1', '4', '9', '9', '3', '3', '4', '9']\n",
      "tensor([9, 3, 0, 2, 3, 3, 4, 9, 9, 2, 3, 4, 4, 9, 1, 1])\n",
      "['9', '3', '0', '2', '5', '3', '0', '5', '1', '2', '0', '4', '4', '5', '1', '4']\n",
      "tensor([9, 3, 8, 9, 4, 4, 1, 9, 0, 9, 0, 9, 3, 9, 9, 2])\n",
      "['1', '3', '8', '9', '4', '3', '1', '9', '0', '9', '0', '9', '3', '3', '9', '2']\n",
      "tensor([9, 0, 3, 9, 3, 3, 9, 2, 9, 4, 4, 4, 9, 8, 9, 9])\n",
      "['9', '5', '5', '9', '3', '0', '0', '9', '9', '4', '4', '4', '1', '8', '8', '9']\n",
      "tensor([0, 9, 1, 9, 9, 8, 1, 4, 1, 9, 3, 3, 8, 9, 3, 0])\n",
      "['1', '9', '9', '9', '9', '8', '4', '4', '1', '9', '5', '3', '4', '9', '0', '0']\n",
      "tensor([3, 3, 4, 4, 4, 1, 3, 4, 1, 9, 3, 0, 3, 9, 9, 9])\n",
      "['6', '3', '4', '4', '4', '1', '3', '4', '1', '1', '3', '0', '1', '9', '9', '9']\n",
      "tensor([3, 9, 9, 9, 4, 9, 9, 3, 3, 9, 4, 9, 9, 2, 3, 4])\n",
      "['0', '8', '9', '3', '9', '1', '6', '3', '1', '5', '4', '9', '9', '2', '3', '4']\n",
      "tensor([9, 2, 9, 3, 3, 4, 8, 4, 3, 9, 9, 9, 3, 4, 9, 9])\n",
      "['9', '2', '4', '3', '5', '4', '8', '4', '0', '9', '0', '9', '3', '4', '9', '1']\n",
      "tensor([9, 4, 4, 0, 9, 9, 4, 0, 9, 4, 3, 1, 3, 9, 4, 0])\n",
      "['0', '4', '4', '9', '0', '9', '4', '0', '9', '8', '3', '0', '0', '9', '4', '1']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_batch, test_current_pos \u001b[38;5;129;01min\u001b[39;00m get_batch(bs, max_doc_len, test_texts, smaller_word_dict):\n\u001b[0;32m----> 9\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_current_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m# get max of out\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[11], line 47\u001b[0m, in \u001b[0;36mClassificationAverageModel.forward\u001b[0;34m(self, batch, current_pos)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[1;32m     46\u001b[0m     doc_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup[doc_ids\u001b[38;5;241m.\u001b[39mtolist()] \n\u001b[0;32m---> 47\u001b[0m     e_ds\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39msum(\u001b[43mdoc_embeds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocs[current_pos\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs\u001b[38;5;241m+\u001b[39mi]))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# return self.softmax(self.linear(torch.from_numpy(np.array(e_ds)).float()))\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(e_ds))\u001b[38;5;241m.\u001b[39mfloat())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test Set Evaluation (1 point): After finishing the training, load the (already stored) best performing model, and use it for class prediction on the test set.\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.eval()\n",
    "model = ClassificationAverageModel(word_vecs, test_texts, bs)\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "for test_batch, test_current_pos in get_batch(bs, max_doc_len, test_texts, smaller_word_dict):\n",
    "            out = model(test_batch, test_current_pos)\n",
    "            # get max of out\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            print(pred)\n",
    "            print(test_labels[test_current_pos-bs:test_current_pos])\n",
    "            \n",
    "# Reporting (1 point): During loading and processing the collection, provide sufficient information and examples about the data and the applied processing steps. Report the results of the best performing model on the validation and test set in a table.\n",
    "# TODO TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskB\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Document Classification with BERT (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task aims to conduct the same document classification as Task A, but now by utilizing a pre-trained BERT model. Feel free to reuse any code from the previous task. The implementation of the classifier should cover the points below.\n",
    "\n",
    "**Loading BERT model (2 points):** Use the `transformers` library from `huggingface` to load a (small) pre-trained BERT model. Select a BERT model according to your available resources. The available models can be found [here](https://huggingface.co/models) and [here](https://github.com/google-research/bert).\n",
    "\n",
    "**BERT tokenization (3 points):** For training BERT models, we do not need to create a dictionary anymore, as a BERT model already contains an internal subword dictionary. Following the instruction in `transformers`'s documentation, tokenize the data using the BERT model.  \n",
    "\n",
    "**Model definition and forward function (5 points):** Define the class **`ClassificationBERTModel`** as a PyTorch model. In the initialization procedure, the model receives the loaded BERT model and stores it as the model's parameter. The parameters of the BERT model should be trainable. The forward function of the model receives a batch of data, passes this batch to BERT, and achieves the corresponding document embeddings from the output of BERT. Similar to the previous task, the document embeddings are used for classification by linearly transforming document embeddings to the vectors with the number of classes, followed by applying Softmax.\n",
    "\n",
    "**Training and overall functionality (3 points):** Train the model in a similar fashion to the previous task, namely with the proper loss function, optimization, and early stoping.\n",
    "\n",
    "**Test Set Evaluation (1 point):** After finishing the training, load the (already stored) best performing model, and use it for class prediction on the test set.\n",
    "\n",
    "**Reporting (1 point):** Report the results of the best performing model on the validation and test set in a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading BERT model (2 points): \n",
    "# Use the transformers library from huggingface to load a (small) pre-trained BERT model. \n",
    "# Select a BERT model according to your available resources. The available models can be found here and here.\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenization (3 points): \n",
    "# For training BERT models, we do not need to create a dictionary anymore, \n",
    "# as a BERT model already contains an internal subword dictionary. \n",
    "# Following the instruction in transformers's documentation, \n",
    "# tokenize the data using the BERT model.\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "# print(tokenizer(train_texts[0], padding='max_length', truncation=True, max_length=80))      \n",
    "\n",
    "def get_batch(bs, max_doc_len, dataset):\n",
    "    \n",
    "    batch = np.zeros((bs, max_doc_len))\n",
    "    attention_mask = np.zeros((bs, max_doc_len))\n",
    "    current_pos = 0\n",
    "    mult_factor = 0\n",
    "    \n",
    "    for i, data_point in enumerate(dataset):\n",
    "        if i+bs >= len(dataset):\n",
    "            break\n",
    "        if i % bs == 0 and i > 0:\n",
    "            current_pos += bs\n",
    "            yield torch.IntTensor(batch), torch.IntTensor(attention_mask), current_pos\n",
    "            mult_factor += 1\n",
    "     \n",
    "        batch[i-(bs*mult_factor)] = tokenizer(data_point, padding='max_length', truncation=True, max_length=80)['input_ids']\n",
    "        attention_mask[i-(bs*mult_factor)]= tokenizer(data_point, padding='max_length', truncation=True, max_length=80)['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO try different versions----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7083, grad_fn=<NllLossBackward0>)\n",
      "0.7461180124223602\n",
      "0.7833850931677019\n",
      "0.7934782608695652\n",
      "0.8043478260869565\n",
      "Problem\n",
      "Problem\n",
      "Problem\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m max_doc_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch, mask, current_pos) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(get_batch(bs, max_doc_len, train_texts)):\n\u001b[1;32m     59\u001b[0m         optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     60\u001b[0m         out \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mforward(batch, mask)          \n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(bs, max_doc_len, dataset)\u001b[0m\n\u001b[1;32m     24\u001b[0m     mult_factor \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m batch[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)] \u001b[38;5;241m=\u001b[39m tokenizer(data_point, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m attention_mask[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)]\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:245\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    243\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    250\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:423\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[0;32m--> 423\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:525\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    524\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_is_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model definition and forward function (5 points): \n",
    "# Define the class ClassificationBERTModel as a PyTorch model. \n",
    "# In the initialization procedure, the model receives the loaded BERT model \n",
    "# and stores it as the model's parameter. \n",
    "# The parameters of the BERT model should be trainable. \n",
    "# The forward function of the model receives a batch of data, passes this batch to BERT, \n",
    "# and achieves the corresponding document embeddings from the output of BERT. \n",
    "# Similar to the previous task, the document embeddings are used for classification\n",
    "# by linearly transforming document embeddings to the vectors with the number of classes, \n",
    "# followed by applying Softmax.\n",
    "\n",
    "class ClassificationBERTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, train=True):\n",
    "        super(ClassificationBERTModel, self).__init__()\n",
    "        self.model = model # nn.Parameter(model)\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # self.model.requires_grad = True\n",
    "        self.linear = nn.Linear(128,12)# smaller cuz of tiny bert, else it is around 700\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, batch, attention_mask):\n",
    "        # batch are the input_ids of the texts...\n",
    "        # out = self.softmax(self.linear(self.model(batch)['pooler_output']))\n",
    "        # TODO pooler_output???\n",
    "        # print(self.model(batch)['pooler_output'].shape)\n",
    "        out = self.linear(self.model(batch, attention_mask)['pooler_output'])\n",
    "        return out\n",
    "        \n",
    "        # print(out['last_hidden_state'].shape, out['pooler_output'].shape)\n",
    "        \n",
    "# Training and overall functionality (3 points): \n",
    "# Train the model in a similar fashion to the previous task, \n",
    "# namely with the proper loss function, optimization, and early stoping.\n",
    "    \n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\") \n",
    "\n",
    "classifier = ClassificationBERTModel(model)\n",
    "loss_fct = nn.CrossEntropyLoss() # nn.NLLLoss()\n",
    "# lr=2e-5, eps=1e-8\n",
    "optim = torch.optim.Adam(classifier.parameters(), 5e-5)\n",
    "\n",
    "old_acc = 0\n",
    "nr_epochs = 60\n",
    "count = 0\n",
    "bs = 16\n",
    "max_doc_len = 80\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for i, (batch, mask, current_pos) in enumerate(get_batch(bs, max_doc_len, train_texts)):\n",
    "        optim.zero_grad()\n",
    "        out = classifier.forward(batch, mask)          \n",
    "        labels = train_labels[current_pos-bs:current_pos]            \n",
    "        loss = loss_fct(out, torch.tensor(np.array(labels).astype('int')))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(loss)\n",
    "            \n",
    "    # early stopping at the end of every epoch\n",
    "    acc = 0\n",
    "    nr_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch, mask, val_current_pos in get_batch(bs, max_doc_len, val_texts):\n",
    "            out = classifier(val_batch, mask)\n",
    "            # get max of out\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            acc += (torch.sum(pred == torch.tensor(np.array(val_labels[val_current_pos-bs:val_current_pos]).astype('int'))).item() / bs)\n",
    "            nr_batches += 1\n",
    "        acc /= nr_batches\n",
    "        count +=1\n",
    "        if acc > old_acc:\n",
    "            old_acc = acc\n",
    "            print(acc)\n",
    "            torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "            count = 0\n",
    "        if count > 10:\n",
    "            # end training\n",
    "            # break  \n",
    "            print('Problem') \n",
    "            \n",
    "# TODO can add dropout, weight decay...if training loss decreases, but accuracy does not improve\n",
    "# we overfit on the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9907, grad_fn=<NllLossBackward0>)\n",
      "0.30240683229813664\n",
      "0.46467391304347827\n",
      "0.5512422360248447\n",
      "0.6075310559006211\n",
      "0.6630434782608695\n",
      "0.687111801242236\n",
      "0.703416149068323\n",
      "0.7267080745341615\n",
      "0.7294254658385093\n",
      "0.7360248447204969\n",
      "0.7383540372670807\n",
      "0.7527173913043478\n",
      "0.7678571428571429\n",
      "0.7690217391304348\n",
      "0.7787267080745341\n",
      "tensor(0.5698, grad_fn=<NllLossBackward0>)\n",
      "0.779891304347826\n",
      "0.781832298136646\n",
      "0.7833850931677019\n",
      "tensor(0.3412, grad_fn=<NllLossBackward0>)\n",
      "Problem\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m max_doc_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch, mask, current_pos) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(get_batch(bs, max_doc_len, train_texts)):\n\u001b[1;32m     59\u001b[0m         optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     60\u001b[0m         out \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mforward(batch, mask)          \n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(bs, max_doc_len, dataset)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mIntTensor(batch), torch\u001b[38;5;241m.\u001b[39mIntTensor(attention_mask), current_pos\n\u001b[1;32m     24\u001b[0m     mult_factor \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 26\u001b[0m batch[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m attention_mask[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)]\u001b[38;5;241m=\u001b[39m tokenizer(data_point, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:245\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    243\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    250\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:423\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;66;03m# union() returns a new set by concatenating the two sets.\u001b[39;00m\n\u001b[1;32m    422\u001b[0m never_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\u001b[38;5;241m.\u001b[39munion(\u001b[38;5;28mset\u001b[39m(never_split)) \u001b[38;5;28;01mif\u001b[39;00m never_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnever_split\n\u001b[0;32m--> 423\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:525\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    524\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m--> 525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cp \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFFFD\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_is_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    526\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_whitespace(char):\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:283\u001b[0m, in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_is_control\u001b[39m(char):\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether `char` is a control character.\"\"\"\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# These are technically control characters but we count them as whitespace\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# characters.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model definition and forward function (5 points): \n",
    "# Define the class ClassificationBERTModel as a PyTorch model. \n",
    "# In the initialization procedure, the model receives the loaded BERT model \n",
    "# and stores it as the model's parameter. \n",
    "# The parameters of the BERT model should be trainable. \n",
    "# The forward function of the model receives a batch of data, passes this batch to BERT, \n",
    "# and achieves the corresponding document embeddings from the output of BERT. \n",
    "# Similar to the previous task, the document embeddings are used for classification\n",
    "# by linearly transforming document embeddings to the vectors with the number of classes, \n",
    "# followed by applying Softmax.\n",
    "\n",
    "class ClassificationBERTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, train=True):\n",
    "        super(ClassificationBERTModel, self).__init__()\n",
    "        self.model = model # nn.Parameter(model)\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # self.model.requires_grad = True\n",
    "        self.linear = nn.Linear(128,12)# smaller cuz of tiny bert, else it is around 700\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, batch, attention_mask):\n",
    "        # batch are the input_ids of the texts...\n",
    "        # out = self.softmax(self.linear(self.model(batch)['pooler_output']))\n",
    "        # TODO pooler_output???\n",
    "        # print(self.model(batch)['pooler_output'].shape)\n",
    "        out = self.linear(self.model(batch, attention_mask)['pooler_output'])\n",
    "        return out\n",
    "        \n",
    "        # print(out['last_hidden_state'].shape, out['pooler_output'].shape)\n",
    "        \n",
    "# Training and overall functionality (3 points): \n",
    "# Train the model in a similar fashion to the previous task, \n",
    "# namely with the proper loss function, optimization, and early stoping.\n",
    "    \n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\", hidden_dropout_prob=0.5) \n",
    "\n",
    "classifier = ClassificationBERTModel(model)\n",
    "loss_fct = nn.CrossEntropyLoss() # nn.NLLLoss()\n",
    "# lr=2e-5, eps=1e-8\n",
    "optim = torch.optim.Adam(classifier.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "\n",
    "old_acc = 0\n",
    "nr_epochs = 60\n",
    "count = 0\n",
    "bs = 16\n",
    "max_doc_len = 80\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for i, (batch, mask, current_pos) in enumerate(get_batch(bs, max_doc_len, train_texts)):\n",
    "        optim.zero_grad()\n",
    "        out = classifier.forward(batch, mask)          \n",
    "        labels = train_labels[current_pos-bs:current_pos]            \n",
    "        loss = loss_fct(out, torch.tensor(np.array(labels).astype('int')))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(loss)\n",
    "            \n",
    "    # early stopping at the end of every epoch\n",
    "    acc = 0\n",
    "    nr_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch, mask, val_current_pos in get_batch(bs, max_doc_len, val_texts):\n",
    "            out = classifier(val_batch, mask)\n",
    "            # get max of out\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            acc += (torch.sum(pred == torch.tensor(np.array(val_labels[val_current_pos-bs:val_current_pos]).astype('int'))).item() / bs)\n",
    "            nr_batches += 1\n",
    "        acc /= nr_batches\n",
    "        count +=1\n",
    "        if acc > old_acc:\n",
    "            old_acc = acc\n",
    "            print(acc)\n",
    "            torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "            count = 0\n",
    "        if count > 10:\n",
    "            # end training\n",
    "            # break  \n",
    "            print('Problem') \n",
    "            \n",
    "# TODO can add dropout, weight decay...if training loss decreases, but accuracy does not improve\n",
    "# we overfit on the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3069, grad_fn=<NllLossBackward0>)\n",
      "0.33812111801242234\n",
      "0.4010093167701863\n",
      "0.41847826086956524\n",
      "0.4277950310559006\n",
      "0.43827639751552794\n",
      "0.5543478260869565\n",
      "0.5613354037267081\n",
      "0.562888198757764\n",
      "0.5656055900621118\n",
      "0.5753105590062112\n",
      "0.5756987577639752\n",
      "0.5795807453416149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m\n\u001b[1;32m     55\u001b[0m max_doc_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nr_epochs):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (batch, mask, current_pos) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(get_batch(bs, max_doc_len, train_texts)):\n\u001b[1;32m     59\u001b[0m         optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     60\u001b[0m         out \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mforward(batch, mask)          \n",
      "Cell \u001b[0;32mIn[7], line 27\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(bs, max_doc_len, dataset)\u001b[0m\n\u001b[1;32m     24\u001b[0m     mult_factor \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m batch[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)] \u001b[38;5;241m=\u001b[39m tokenizer(data_point, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m attention_mask[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)]\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:617\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:245\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    243\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 245\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    250\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:432\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n\u001b[0;32m--> 432\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_chinese_chars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# prevents treating the same character with different unicode codepoints as different characters\u001b[39;00m\n\u001b[1;32m    434\u001b[0m unicode_normalized_text \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFC\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:488\u001b[0m, in \u001b[0;36mBasicTokenizer._tokenize_chinese_chars\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    487\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_chinese_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    489\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    490\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(char)\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py:513\u001b[0m, in \u001b[0;36mBasicTokenizer._is_chinese_char\u001b[0;34m(self, cp)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# This defines a \"chinese character\" as anything in the CJK Unicode block:\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\u001b[39;00m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# space-separated words, so they are not treated specially and handled\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# like the all of the other languages.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    507\u001b[0m     (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x4E00\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x9FFF\u001b[39m)\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x3400\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x4DBF\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x20000\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2A6DF\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2A700\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B73F\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B740\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B81F\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B820\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2CEAF\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0xF900\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0xFAFF\u001b[39m)\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2F800\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2FA1F\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    515\u001b[0m ):  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model definition and forward function (5 points): \n",
    "# Define the class ClassificationBERTModel as a PyTorch model. \n",
    "# In the initialization procedure, the model receives the loaded BERT model \n",
    "# and stores it as the model's parameter. \n",
    "# The parameters of the BERT model should be trainable. \n",
    "# The forward function of the model receives a batch of data, passes this batch to BERT, \n",
    "# and achieves the corresponding document embeddings from the output of BERT. \n",
    "# Similar to the previous task, the document embeddings are used for classification\n",
    "# by linearly transforming document embeddings to the vectors with the number of classes, \n",
    "# followed by applying Softmax.\n",
    "\n",
    "class ClassificationBERTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, train=True):\n",
    "        super(ClassificationBERTModel, self).__init__()\n",
    "        self.model = model # nn.Parameter(model)\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # self.model.requires_grad = True\n",
    "        self.linear = nn.Linear(128,12)# smaller cuz of tiny bert, else it is around 700\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, batch, attention_mask):\n",
    "        # batch are the input_ids of the texts...\n",
    "        # out = self.softmax(self.linear(self.model(batch)['pooler_output']))\n",
    "        # TODO pooler_output???\n",
    "        # print(self.model(batch)['pooler_output'].shape)\n",
    "        out = self.softmax(self.linear(self.model(batch, attention_mask)['pooler_output']))\n",
    "        return out\n",
    "        \n",
    "        # print(out['last_hidden_state'].shape, out['pooler_output'].shape)\n",
    "        \n",
    "# Training and overall functionality (3 points): \n",
    "# Train the model in a similar fashion to the previous task, \n",
    "# namely with the proper loss function, optimization, and early stoping.\n",
    "    \n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\", hidden_dropout_prob=0.5) \n",
    "\n",
    "classifier = ClassificationBERTModel(model)\n",
    "loss_fct = nn.NLLLoss() # nn.NLLLoss()\n",
    "# lr=2e-5, eps=1e-8\n",
    "optim = torch.optim.Adam(classifier.parameters(), lr=2e-5, weight_decay=1e-5)\n",
    "\n",
    "old_acc = 0\n",
    "nr_epochs = 60\n",
    "count = 0\n",
    "bs = 16\n",
    "max_doc_len = 80\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for i, (batch, mask, current_pos) in enumerate(get_batch(bs, max_doc_len, train_texts)):\n",
    "        optim.zero_grad()\n",
    "        out = classifier.forward(batch, mask)          \n",
    "        labels = train_labels[current_pos-bs:current_pos]            \n",
    "        loss = loss_fct(out, torch.tensor(np.array(labels).astype('int')))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(loss)\n",
    "            \n",
    "    # early stopping at the end of every epoch\n",
    "    acc = 0\n",
    "    nr_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch, mask, val_current_pos in get_batch(bs, max_doc_len, val_texts):\n",
    "            out = classifier(val_batch, mask)\n",
    "            # get max of out\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            acc += (torch.sum(pred == torch.tensor(np.array(val_labels[val_current_pos-bs:val_current_pos]).astype('int'))).item() / bs)\n",
    "            nr_batches += 1\n",
    "        acc /= nr_batches\n",
    "        count +=1\n",
    "        if acc > old_acc:\n",
    "            old_acc = acc\n",
    "            print(acc)\n",
    "            torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "            count = 0\n",
    "        if count > 10:\n",
    "            # end training\n",
    "            # break  \n",
    "            print('Problem') \n",
    "            \n",
    "# TODO can add dropout, weight decay...if training loss decreases, but accuracy does not improve\n",
    "# we overfit on the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5557, grad_fn=<NllLossBackward0>)\n",
      "0.5714285714285714\n",
      "0.6482919254658385\n",
      "0.6583850931677019\n",
      "0.6622670807453416\n",
      "0.6909937888198758\n",
      "0.7010869565217391\n",
      "0.7263198757763976\n",
      "0.7430124223602484\n",
      "0.7449534161490683\n",
      "0.765527950310559\n",
      "0.7690217391304348\n",
      "tensor(-0.9336, grad_fn=<NllLossBackward0>)\n",
      "0.7697981366459627\n",
      "0.7721273291925466\n",
      "tensor(-0.9370, grad_fn=<NllLossBackward0>)\n",
      "Problem\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m     labels \u001b[38;5;241m=\u001b[39m train_labels[current_pos\u001b[38;5;241m-\u001b[39mbs:current_pos]            \n\u001b[1;32m     62\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fct(out, torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(labels)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model definition and forward function (5 points): \n",
    "# Define the class ClassificationBERTModel as a PyTorch model. \n",
    "# In the initialization procedure, the model receives the loaded BERT model \n",
    "# and stores it as the model's parameter. \n",
    "# The parameters of the BERT model should be trainable. \n",
    "# The forward function of the model receives a batch of data, passes this batch to BERT, \n",
    "# and achieves the corresponding document embeddings from the output of BERT. \n",
    "# Similar to the previous task, the document embeddings are used for classification\n",
    "# by linearly transforming document embeddings to the vectors with the number of classes, \n",
    "# followed by applying Softmax.\n",
    "\n",
    "class ClassificationBERTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, train=True):\n",
    "        super(ClassificationBERTModel, self).__init__()\n",
    "        self.model = model # nn.Parameter(model)\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # self.model.requires_grad = True\n",
    "        self.linear = nn.Linear(128,12)# smaller cuz of tiny bert, else it is around 700\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, batch, attention_mask):\n",
    "        # batch are the input_ids of the texts...\n",
    "        # out = self.softmax(self.linear(self.model(batch)['pooler_output']))\n",
    "        # TODO pooler_output???\n",
    "        # print(self.model(batch)['pooler_output'].shape)\n",
    "        out = self.softmax(self.linear(self.model(batch, attention_mask)['pooler_output']))\n",
    "        return out\n",
    "        \n",
    "        # print(out['last_hidden_state'].shape, out['pooler_output'].shape)\n",
    "        \n",
    "# Training and overall functionality (3 points): \n",
    "# Train the model in a similar fashion to the previous task, \n",
    "# namely with the proper loss function, optimization, and early stoping.\n",
    "    \n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\") \n",
    "\n",
    "classifier = ClassificationBERTModel(model)\n",
    "loss_fct = nn.NLLLoss() # nn.NLLLoss()\n",
    "# lr=2e-5, eps=1e-8\n",
    "optim = torch.optim.Adam(classifier.parameters(), lr=2e-5, weight_decay=1e-4)\n",
    "\n",
    "old_acc = 0\n",
    "nr_epochs = 60\n",
    "count = 0\n",
    "bs = 16\n",
    "max_doc_len = 80\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for i, (batch, mask, current_pos) in enumerate(get_batch(bs, max_doc_len, train_texts)):\n",
    "        optim.zero_grad()\n",
    "        out = classifier.forward(batch, mask)          \n",
    "        labels = train_labels[current_pos-bs:current_pos]            \n",
    "        loss = loss_fct(out, torch.tensor(np.array(labels).astype('int')))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(loss)\n",
    "            \n",
    "    # early stopping at the end of every epoch\n",
    "    acc = 0\n",
    "    nr_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch, mask, val_current_pos in get_batch(bs, max_doc_len, val_texts):\n",
    "            out = classifier(val_batch, mask)\n",
    "            # get max of out\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            acc += (torch.sum(pred == torch.tensor(np.array(val_labels[val_current_pos-bs:val_current_pos]).astype('int'))).item() / bs)\n",
    "            nr_batches += 1\n",
    "        acc /= nr_batches\n",
    "        count +=1\n",
    "        if acc > old_acc:\n",
    "            old_acc = acc\n",
    "            print(acc)\n",
    "            torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "            count = 0\n",
    "        if count > 10:\n",
    "            # end training\n",
    "            # break  \n",
    "            print('Problem') \n",
    "            \n",
    "# TODO can add dropout, weight decay...if training loss decreases, but accuracy does not improve\n",
    "# we overfit on the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1101, grad_fn=<NllLossBackward0>)\n",
      "0.296583850931677\n",
      "0.35403726708074534\n",
      "0.3606366459627329\n",
      "0.3769409937888199\n",
      "0.3796583850931677\n",
      "0.38742236024844723\n",
      "0.39712732919254656\n",
      "0.3998447204968944\n",
      "0.4169254658385093\n",
      "0.4231366459627329\n",
      "0.4359472049689441\n",
      "0.4704968944099379\n",
      "0.48330745341614906\n",
      "0.4891304347826087\n",
      "0.4957298136645963\n",
      "0.5069875776397516\n",
      "0.5143633540372671\n",
      "0.5194099378881988\n",
      "tensor(1.6919, grad_fn=<NllLossBackward0>)\n",
      "0.5248447204968945\n",
      "0.5267857142857143\n",
      "0.5407608695652174\n",
      "0.5508540372670807\n",
      "0.5539596273291926\n",
      "0.5617236024844721\n",
      "0.5687111801242236\n",
      "0.5788043478260869\n",
      "0.5834627329192547\n",
      "0.5989906832298136\n",
      "0.6110248447204969\n",
      "0.6203416149068323\n",
      "0.6370341614906833\n",
      "0.640139751552795\n",
      "0.6413043478260869\n",
      "0.6502329192546584\n",
      "0.6521739130434783\n",
      "0.6587732919254659\n",
      "0.6704192546583851\n",
      "tensor(1.0591, grad_fn=<NllLossBackward0>)\n",
      "0.671972049689441\n",
      "0.6750776397515528\n",
      "0.6843944099378882\n",
      "0.6890527950310559\n",
      "0.6894409937888198\n",
      "0.6964285714285714\n",
      "0.6975931677018633\n",
      "0.7041925465838509\n",
      "0.7061335403726708\n",
      "0.7127329192546584\n",
      "0.7138975155279503\n"
     ]
    }
   ],
   "source": [
    "# Model definition and forward function (5 points): \n",
    "# Define the class ClassificationBERTModel as a PyTorch model. \n",
    "# In the initialization procedure, the model receives the loaded BERT model \n",
    "# and stores it as the model's parameter. \n",
    "# The parameters of the BERT model should be trainable. \n",
    "# The forward function of the model receives a batch of data, passes this batch to BERT, \n",
    "# and achieves the corresponding document embeddings from the output of BERT. \n",
    "# Similar to the previous task, the document embeddings are used for classification\n",
    "# by linearly transforming document embeddings to the vectors with the number of classes, \n",
    "# followed by applying Softmax.\n",
    "\n",
    "class ClassificationBERTModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, train=True):\n",
    "        super(ClassificationBERTModel, self).__init__()\n",
    "        self.model = model # nn.Parameter(model)\n",
    "        if train:\n",
    "            self.model.train()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # self.model.requires_grad = True\n",
    "        self.linear = nn.Linear(128,12)# smaller cuz of tiny bert, else it is around 700\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, batch, attention_mask):\n",
    "        # batch are the input_ids of the texts...\n",
    "        # out = self.softmax(self.linear(self.model(batch)['pooler_output']))\n",
    "        # TODO pooler_output???\n",
    "        # print(self.model(batch)['pooler_output'].shape)\n",
    "        out = self.linear(self.model(batch, attention_mask)['pooler_output'])\n",
    "        return out\n",
    "        \n",
    "        # print(out['last_hidden_state'].shape, out['pooler_output'].shape)\n",
    "        \n",
    "# Training and overall functionality (3 points): \n",
    "# Train the model in a similar fashion to the previous task, \n",
    "# namely with the proper loss function, optimization, and early stoping.\n",
    "    \n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\") \n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\") \n",
    "\n",
    "classifier = ClassificationBERTModel(model)\n",
    "loss_fct = nn.CrossEntropyLoss() # nn.NLLLoss()\n",
    "# lr=2e-5, eps=1e-8\n",
    "optim = torch.optim.SGD(classifier.parameters(), lr=5e-5)\n",
    "\n",
    "old_acc = 0\n",
    "nr_epochs = 60\n",
    "count = 0\n",
    "bs = 16\n",
    "max_doc_len = 80\n",
    "\n",
    "for epoch in range(nr_epochs):\n",
    "    for i, (batch, mask, current_pos) in enumerate(get_batch(bs, max_doc_len, train_texts)):\n",
    "        optim.zero_grad()\n",
    "        out = classifier.forward(batch, mask)          \n",
    "        labels = train_labels[current_pos-bs:current_pos]            \n",
    "        loss = loss_fct(out, torch.tensor(np.array(labels).astype('int')))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(loss)\n",
    "            \n",
    "    # early stopping at the end of every epoch\n",
    "    acc = 0\n",
    "    nr_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for val_batch, mask, val_current_pos in get_batch(bs, max_doc_len, val_texts):\n",
    "            out = classifier(val_batch, mask)\n",
    "            # get max of out\n",
    "            pred = torch.argmax(out, dim=1)\n",
    "            acc += (torch.sum(pred == torch.tensor(np.array(val_labels[val_current_pos-bs:val_current_pos]).astype('int'))).item() / bs)\n",
    "            nr_batches += 1\n",
    "        acc /= nr_batches\n",
    "        count +=1\n",
    "        if acc > old_acc:\n",
    "            old_acc = acc\n",
    "            print(acc)\n",
    "            torch.save(classifier.state_dict(), 'best_model.pth')\n",
    "            count = 0\n",
    "        if count > 10:\n",
    "            # end training\n",
    "            # break  \n",
    "            print('Problem') \n",
    "            \n",
    "# TODO can add dropout, weight decay...if training loss decreases, but accuracy does not improve\n",
    "# we overfit on the training data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 4, 3, 9, 9, 1, 3, 3, 3, 4, 4, 3, 9, 9, 2, 4])\n",
      "['8', '9', '3', '9', '8', '1', '3', '3', '0', '1', '4', '3', '4', '9', '2', '4']\n",
      "tensor([4, 4, 1, 3, 4, 9, 4, 3, 3, 9, 9, 9, 4, 9, 4, 4])\n",
      "['4', '4', '1', '5', '4', '9', '4', '5', '3', '0', '9', '9', '4', '9', '4', '4']\n",
      "tensor([3, 2, 3, 9, 4, 0, 9, 9, 4, 9, 4, 9, 0, 3, 9, 4])\n",
      "['8', '2', '3', '4', '4', '8', '9', '9', '4', '9', '4', '9', '0', '3', '9', '4']\n",
      "tensor([2, 1, 3, 9, 3, 9, 0, 9, 1, 4, 9, 9, 4, 4, 0, 4])\n",
      "['2', '1', '3', '4', '5', '9', '0', '9', '1', '4', '9', '9', '8', '4', '0', '4']\n",
      "tensor([3, 3, 3, 8, 2, 4, 3, 9, 0, 9, 9, 4, 9, 4, 0, 4])\n",
      "['3', '7', '3', '8', '2', '4', '3', '9', '9', '1', '9', '4', '0', '4', '9', '4']\n",
      "tensor([9, 0, 3, 9, 0, 1, 1, 3, 0, 9, 3, 0, 9, 4, 9, 4])\n",
      "['9', '0', '3', '9', '9', '1', '9', '3', '2', '9', '3', '9', '9', '4', '5', '4']\n",
      "tensor([9, 4, 9, 4, 9, 0, 3, 1, 3, 4, 4, 2, 4, 1, 3, 3])\n",
      "['9', '4', '9', '4', '6', '0', '3', '0', '3', '4', '4', '2', '1', '1', '3', '3']\n",
      "tensor([4, 2, 3, 1, 4, 1, 0, 4, 9, 3, 3, 1, 3, 9, 4, 4])\n",
      "['4', '1', '3', '1', '4', '1', '6', '4', '6', '0', '3', '1', '3', '9', '8', '8']\n",
      "tensor([9, 3, 1, 0, 3, 9, 4, 3, 3, 3, 9, 3, 9, 8, 3, 1])\n",
      "['9', '5', '1', '0', '3', '3', '9', '3', '8', '3', '8', '3', '9', '8', '3', '1']\n",
      "tensor([3, 9, 3, 2, 9, 3, 3, 3, 4, 4, 9, 0, 9, 1, 3, 3])\n",
      "['3', '9', '5', '6', '1', '5', '3', '3', '4', '7', '9', '9', '9', '1', '3', '7']\n",
      "tensor([3, 9, 2, 4, 3, 4, 4, 9, 4, 9, 3, 4, 9, 3, 1, 4])\n",
      "['5', '3', '2', '4', '3', '4', '4', '9', '4', '9', '3', '4', '9', '0', '1', '4']\n",
      "tensor([9, 3, 9, 8, 4, 3, 2, 9, 2, 9, 9, 3, 4, 4, 9, 4])\n",
      "['5', '5', '9', '8', '4', '3', '2', '1', '2', '4', '9', '1', '4', '4', '9', '4']\n",
      "tensor([4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 0, 3, 1, 3, 4, 2])\n",
      "['4', '3', '4', '3', '4', '3', '4', '4', '4', '4', '0', '5', '1', '3', '4', '2']\n",
      "tensor([9, 9, 3, 9, 4, 9, 9, 9, 3, 4, 4, 1, 4, 3, 2, 4])\n",
      "['9', '9', '9', '9', '4', '9', '9', '9', '3', '4', '4', '9', '4', '3', '9', '4']\n",
      "tensor([9, 9, 1, 3, 4, 2, 1, 2, 9, 0, 4, 3, 3, 9, 4, 3])\n",
      "['9', '9', '1', '5', '4', '2', '1', '0', '9', '0', '0', '8', '5', '4', '4', '3']\n",
      "tensor([4, 4, 4, 3, 9, 0, 9, 4, 4, 3, 3, 4, 9, 9, 1, 3])\n",
      "['4', '7', '4', '3', '0', '0', '6', '4', '4', '3', '5', '4', '9', '9', '1', '3']\n",
      "tensor([9, 4, 4, 3, 0, 9, 9, 3, 9, 1, 4, 9, 3, 1, 9, 2])\n",
      "['5', '4', '4', '1', '0', '3', '9', '3', '9', '1', '4', '9', '5', '0', '9', '2']\n",
      "tensor([4, 4, 9, 9, 4, 4, 1, 4, 4, 1, 4, 9, 0, 3, 3, 4])\n",
      "['4', '4', '9', '1', '4', '4', '1', '4', '4', '1', '4', '0', '0', '9', '3', '4']\n",
      "tensor([8, 3, 9, 1, 9, 4, 3, 4, 3, 9, 9, 4, 9, 4, 3, 2])\n",
      "['8', '3', '6', '5', '9', '4', '5', '9', '2', '9', '9', '4', '3', '9', '3', '2']\n",
      "tensor([4, 3, 1, 3, 3, 0, 1, 3, 9, 4, 4, 1, 2, 8, 4, 9])\n",
      "['4', '3', '1', '3', '4', '9', '1', '5', '0', '4', '4', '4', '2', '8', '4', '9']\n",
      "tensor([2, 9, 9, 8, 4, 2, 4, 4, 0, 4, 4, 1, 3, 8, 4, 9])\n",
      "['2', '9', '1', '8', '4', '2', '4', '4', '5', '4', '4', '0', '3', '8', '4', '9']\n",
      "tensor([4, 3, 9, 3, 3, 2, 4, 0, 9, 9, 3, 0, 3, 9, 1, 2])\n",
      "['4', '3', '9', '3', '3', '2', '4', '1', '9', '9', '3', '0', '1', '9', '3', '2']\n",
      "tensor([3, 3, 9, 9, 4, 3, 4, 9, 9, 3, 0, 9, 9, 9, 0, 0])\n",
      "['3', '0', '9', '9', '4', '3', '4', '9', '5', '3', '4', '9', '9', '9', '1', '5']\n",
      "tensor([2, 2, 0, 4, 4, 1, 3, 9, 3, 9, 0, 2, 3, 9, 8, 4])\n",
      "['2', '2', '1', '4', '4', '1', '3', '9', '7', '0', '3', '2', '7', '9', '8', '4']\n",
      "tensor([4, 9, 3, 4, 9, 3, 0, 4, 4, 3, 4, 1, 9, 9, 3, 0])\n",
      "['4', '8', '0', '4', '0', '0', '0', '4', '4', '3', '4', '9', '9', '9', '3', '0']\n",
      "tensor([9, 2, 1, 3, 8, 4, 9, 9, 3, 4, 9, 3, 9, 3, 4, 9])\n",
      "['9', '2', '1', '6', '8', '4', '9', '4', '3', '4', '1', '3', '9', '5', '4', '6']\n",
      "tensor([3, 4, 4, 9, 1, 9, 9, 4, 9, 4, 9, 9, 3, 3, 1, 9])\n",
      "['3', '4', '4', '9', '1', '9', '1', '4', '1', '4', '9', '9', '3', '3', '4', '9']\n",
      "tensor([9, 3, 4, 2, 3, 3, 4, 9, 9, 2, 0, 4, 4, 3, 1, 4])\n",
      "['9', '3', '0', '2', '5', '3', '0', '5', '1', '2', '0', '4', '4', '5', '1', '4']\n",
      "tensor([9, 3, 3, 9, 4, 3, 1, 9, 0, 9, 0, 9, 3, 3, 9, 2])\n",
      "['1', '3', '8', '9', '4', '3', '1', '9', '0', '9', '0', '9', '3', '3', '9', '2']\n",
      "tensor([9, 1, 0, 9, 3, 0, 0, 9, 9, 4, 4, 4, 9, 8, 3, 9])\n",
      "['9', '5', '5', '9', '3', '0', '0', '9', '9', '4', '4', '4', '1', '8', '8', '9']\n",
      "tensor([0, 9, 1, 9, 9, 8, 4, 4, 1, 9, 3, 3, 4, 9, 9, 0])\n",
      "['1', '9', '9', '9', '9', '8', '4', '4', '1', '9', '5', '3', '4', '9', '0', '0']\n",
      "tensor([1, 3, 4, 4, 4, 1, 3, 4, 1, 9, 3, 0, 3, 9, 9, 1])\n",
      "['6', '3', '4', '4', '4', '1', '3', '4', '1', '1', '3', '0', '1', '9', '9', '9']\n",
      "tensor([3, 0, 9, 3, 9, 9, 9, 3, 4, 9, 4, 9, 9, 2, 3, 4])\n",
      "['0', '8', '9', '3', '9', '1', '6', '3', '1', '5', '4', '9', '9', '2', '3', '4']\n",
      "tensor([9, 2, 9, 3, 3, 4, 8, 4, 3, 0, 9, 9, 3, 4, 9, 1])\n",
      "['9', '2', '4', '3', '5', '4', '8', '4', '0', '9', '0', '9', '3', '4', '9', '1']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m classifier\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# classifier.eval()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_batch, mask, test_current_pos \u001b[38;5;129;01min\u001b[39;00m get_batch(bs, max_doc_len, test_texts):\n\u001b[1;32m     11\u001b[0m         out \u001b[38;5;241m=\u001b[39m classifier(test_batch, mask)\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# get max of out\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mget_batch\u001b[0;34m(bs, max_doc_len, dataset)\u001b[0m\n\u001b[1;32m     24\u001b[0m     mult_factor \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     26\u001b[0m batch[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)] \u001b[38;5;241m=\u001b[39m tokenizer(data_point, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m attention_mask[i\u001b[38;5;241m-\u001b[39m(bs\u001b[38;5;241m*\u001b[39mmult_factor)]\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:686\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 686\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:573\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    568\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[1;32m    571\u001b[0m     ]\n\u001b[1;32m    572\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 573\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[1;32m    576\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/lib/python3.10/re.py:209\u001b[0m, in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    203\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/jupyter_projects/jupyter_env/lib/python3.10/site-packages/transformers/tokenization_utils.py:573\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.<lambda>\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m    567\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    568\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m    569\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    570\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[1;32m    571\u001b[0m     ]\n\u001b[1;32m    572\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 573\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[1;32m    576\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test Set Evaluation (1 point): After finishing the training, \n",
    "# load the (already stored) best performing model, \n",
    "# and use it for class prediction on the test set.\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\") \n",
    "classifier = ClassificationBERTModel(model, train=False)\n",
    "classifier.load_state_dict(torch.load('best_model.pth'))\n",
    "# classifier.eval()\n",
    "\n",
    "for test_batch, mask, test_current_pos in get_batch(bs, max_doc_len, test_texts):\n",
    "        out = classifier(test_batch, mask)\n",
    "        # get max of out\n",
    "        pred = torch.argmax(out, dim=1)\n",
    "        print(pred)\n",
    "        print(test_labels[test_current_pos-bs:test_current_pos])\n",
    "\n",
    "# Reporting (1 point): Report the results of the best performing model on \n",
    "# the validation and test set in a table."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
