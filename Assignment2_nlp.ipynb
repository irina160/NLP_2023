{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill out the information of your group!\n",
    "\n",
    "| <p style=\"text-align: center;\">First Name</p>  | <p style=\"text-align: center;\">Family Name</p> | Matr.-No. |\n",
    "| ---------------------------------------------- | ---------------------------------------------- | -------- |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n",
    "| <p style=\"text-align: left\">*EDIT!*</p>| <p style=\"text-align: left\">*EDIT!*</p> | *EDIT!* |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center\">344.105/6/7 UE: Natural Language Processing (WS2023/24)</h2>\n",
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 2</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Getting to Know Word Embedding!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Terms of Use</b><br>\n",
    "This  material is prepared for educational purposes at the Johannes Kepler University (JKU) Linz, and is exclusively provided to the registered students of the mentioned course at JKU. It is strictly forbidden to distribute the current file, the contents of the assignment, and its solution. The use or reproduction of this manuscript is only allowed for educational purposes in non-profit organizations, while in this case, the explicit prior acceptance of the author(s) is required.\n",
    "\n",
    "**Authors:** Navid Rekab-saz, Oleg Lesota<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Table of contents</h2>\n",
    "<ol>\n",
    "    <a href=\"#section-general-guidelines\"><li style=\"font-size:large;font-weight:bold\">General Guidelines</li></a>\n",
    "    <a href=\"#section-taskA\"><li style=\"font-size:large;font-weight:bold\">Task A: Words Similarity and Nearest Neighbors (15 points)</li></a>\n",
    "    <a href=\"#section-taskB\"><li style=\"font-size:large;font-weight:bold\">Task B: Document Classification with Word Embedding (15 points)</li></a>\n",
    "    <a href=\"#section-taskC\"><li style=\"font-size:large;font-weight:bold\">Task C: Classification with sent2vec Document Embeddings (2 extra point)</li></a>\n",
    "    <a href=\"#section-references\"><li style=\"font-size:large;font-weight:bold\">References</li></a>\n",
    "    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-general-guidelines\"></a><h2 style=\"color:rgb(0,120,170)\">General Guidelines</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment objective\n",
    "The aim of this assignment is to get familiarized with using word embedding (WE) models in practice. The assignment in total has **30 points**; it also offers **2 extra points** which can cover any missing point.\n",
    "\n",
    "This Notebook encompasses all aspects of the assignment, namely the descriptions of tasks as well as your solutions and reports. Feel free to add any required cell for solutions. The cells can contain code, reports, charts, tables, or any other material, required for the assignment. Feel free to provide the solutions in an interactive and visual way! \n",
    "\n",
    "Please discuss any unclear point in the assignment in the provided forum in MOODLE. It is also encouraged to provide answers to your peer's questions. However when submitting a post, keep in mind to avoid providing solutions. Please let the tutor(s) know shall you find any error or unclarity in the assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries & Dataset\n",
    "\n",
    "The assignment should be implemented with recent versions of `Python` (>3.7). Any standard Python library can be used, so far that the library is free and can be simply installed using `pip` or `conda`. Examples of potentially useful libraries are `scikit-learn`, `numpy`, `scipy`, `gensim`, `nltk`, `spaCy`, and `AllenNLP`. Use the latest stable version of each library.\n",
    "\n",
    "To conduct the experiments, we use a subset of the `HumSet` dataset [1] (https://blog.thedeep.io/humset/). `HumSet` is created by the DEEP (https://www.thedeep.io) project – an open source platform which aims to facilitate processing of textual data for international humanitarian response organizations. The platform enables the classification of text excerpts, extracted from news and reports into a set of domain specific classes. The provided dataset contains the classes (labels) referring to the humanitarian sectors like agriculture, health, and protection. The dataset contains an overall number of 17,301 data points. \n",
    "\n",
    "Download the dataset from the Moodle page of the course.\n",
    "\n",
    "the provided zip file consists of the following files:\n",
    "- `thedeep.subset.train.txt`: Train set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.validation.txt`: Validation set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.test.txt`: Test set in csv format with three fields: sentence_id, text, and label.\n",
    "- `thedeep.subset.label.txt`: Captions of the labels.\n",
    "- `thedeep.ToU.txt`: Terms of use of the dataset.\n",
    "\n",
    "[1] HumSet: Dataset of Multilingual Information Extraction and Classification for Humanitarian Crises Response\n",
    "*Selim Fekih, Nicolo' Tamagnone, Benjamin Minixhofer, Ranjan Shrestha, Ximena Contla, Ewan Oglethorpe and Navid Rekabsaz.* \n",
    "In Findings of the 2022 Conference on Empirical Methods in Natural Language Processing (Findings of EMNLP), December 2022.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Submission\n",
    "\n",
    "Each group should submit the following two files:\n",
    "\n",
    "- One Jupyter Notebook file (`.ipynb`), containing all the code, results, visualizations, etc. **In the submitted Notebook, all the results and visualizations should already be present, and can be observed simply by loading the Notebook in a browser.** The Notebook must be self-contained, meaning that (if necessary) one can run all the cells from top to bottom without any error. Do not forget to put in your names and student numbers in the first cell of the Notebook. \n",
    "- The HTML file (`.html`) achieved from exporting the Jupyter Notebook to HTML (Download As HTML).\n",
    "\n",
    "You do not need to include the data files in the submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskA\"></a><h2 style=\"color:rgb(0,120,170)\">Task A: Words Similarity and Nearest Neighbors (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Loading a word embedding (WE) model (1 points).** Download a pre-trained word embedding model such as word2vec (https://code.google.com/archive/p/word2vec/) or GloVe (https://nlp.stanford.edu/projects/glove/). You can load the downloaded vectors into arrays, or use libraries such as `gensim` to download and process the vectors. \n",
    "\n",
    "**Calculating word-to-word similarities (4 points).** Select <ins>5 arbitrary words</ins> from 5 different topics like objects, science disciplines, verbs, adjectives, animals, etc. Let us refer to these words as *source words*. For each source word, calculate its cosine similarities to <ins>6 target words</ins>. The target words of each source word are also selected by you and should cover various levels of semantic relations – according to your linguistic judgement – to the source word, namely from highly-related to not related at all. Organize the target words in tables, such that the target words of each source word are sorted from the highest to the lowest relevance (according to your judgement). Consider the following points:\n",
    "\n",
    "- **Implementation (2/4 points):** Implement cosine similarity as a function that takes two vectors and returns the similarity score. Implement cosine by yourself and do NOT use the provided functionalities of any library.\n",
    "- **Reporting and observations (2/4 points):** Report the calculated similarities side by side with your word-to-word semantic relevance judgements in tables. Compare the results and report your observations.  \n",
    "\n",
    "**Calculating nearest neighbors (10 points).** For the 5 source words, retrieve the $k=10$ nearest neighbors using the word embedding model, namely the words with the highest similarities to the source word. Consider the following points: \n",
    "    \n",
    "- **Overall implementation (3/10 points):** your implemented function takes a source vector, a set of target vectors, and the $k$ parameter, and returns the $k$ nearest neighbors and their similarity scores. Implement nearest neighbor calculation by yourself and do NOT use the provided functionalities of any library.\n",
    "- **Similarity metrics (2/10 points):** execute the calculation of nearest neighbors according to <ins>two similarity metrics</ins> namely cosine and dot product.\n",
    "- **Efficiency (3/10 points):** your nearest neighbor functions should provide an *efficient* calculation of nearest neighbors. An inefficient way (which should be avoided!) would be looping over the set of vectors in the word embedding model, and one by one calculating the cosine/dot product similarity of the source vector to each of the target vectors. As a hint for an efficient way, consider that in `numpy` (and other libraries), calculating the dot product of a vector to a matrix is much faster than the dot products of the vector to each vector of the matrix.\n",
    "- **Reporting and observations (2/10 points):** report the results in tables, which enable comparing between the outputs of the two similarity metrics. Which similarity metric would you prefer? Report your observations.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD GLOVE VECTORS (funcction)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_glove_vectors(file_path):\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            word_vectors[word] = vector\n",
    "    return word_vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define COSINE SIMILARITY\n",
    "\n",
    "def vec_norm(v):\n",
    "    return sum([x_i**2 for x_i in v])**0.5\n",
    "\n",
    "def cos_similarity(v1, v2):\n",
    "    return np.dot(v1,v2)/(vec_norm(v1)*vec_norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe vectors into a dictionary\n",
    "\n",
    "glove_file_path = 'glove.42B.300d.txt'\n",
    "\n",
    "word_vectors = load_glove_vectors(glove_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the source and target words\n",
    "\n",
    "source_words = [\"cat\", \"nose\", \"pronounced\", \"run\", \"biology\"]\n",
    "target_words = [\"speak\", \"fast\", \"topic\", \"statement\", \"water\", \"paper\"]\n",
    "\n",
    "\n",
    "#the target words for each source word are already ordered in descending order according to the assumed relevance to the respective source word \n",
    "tables = {\"cat\": [\"fast\", \"speak\", \"water\", \"topic\", \"paper\", \"statement\"],\n",
    "          \"nose\": [\"speak\", \"water\", \"topic\", \"statement\", \"paper\",\"fast\"],\n",
    "          \"pronounced\": [\"speak\", \"fast\", \"topic\", \"statement\", \"water\", \"paper\"],\n",
    "          \"run\": [\"fast\", \"water\", \"speak\", \"topic\", \"statement\", \"paper\"],\n",
    "          \"biology\": [\"water\",\"topic\",\"fast\", \"speak\", \"statement\", \"paper\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'cat' has shape (300,)\n",
      "Vector for 'nose' has shape (300,)\n",
      "Vector for 'pronounced' has shape (300,)\n",
      "Vector for 'run' has shape (300,)\n",
      "Vector for 'biology' has shape (300,)\n"
     ]
    }
   ],
   "source": [
    "#Check for target and source words (both has been done) if there are vectors for them in the model\n",
    "for w in source_words:\n",
    "    wv = word_vectors.get(w)\n",
    "    if wv is not None:\n",
    "        print(f\"Vector for '{w}' has shape {wv.shape}\")\n",
    "    else:\n",
    "        print(f\"No vector found for '{w}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': {'paper': 0.3862487796035953,\n",
       "  'fast': 0.3808480561467571,\n",
       "  'water': 0.37276327480506294,\n",
       "  'topic': 0.3257105643374774,\n",
       "  'speak': 0.28739758234400087,\n",
       "  'statement': 0.24221739182178822},\n",
       " 'nose': {'water': 0.32638970830926567,\n",
       "  'fast': 0.29547012211528734,\n",
       "  'speak': 0.28513527554068024,\n",
       "  'paper': 0.2782935277328318,\n",
       "  'topic': 0.25225639384376086,\n",
       "  'statement': 0.1767644612743599},\n",
       " 'pronounced': {'speak': 0.4004911348327155,\n",
       "  'statement': 0.288096391365987,\n",
       "  'fast': 0.2424941116580945,\n",
       "  'water': 0.20449136197396542,\n",
       "  'topic': 0.17967524418090233,\n",
       "  'paper': 0.14073210097205635},\n",
       " 'run': {'fast': 0.5608763791357091,\n",
       "  'water': 0.4897221152131188,\n",
       "  'speak': 0.4398182150191338,\n",
       "  'statement': 0.3798309715676836,\n",
       "  'paper': 0.35950745219297503,\n",
       "  'topic': 0.35072265732266344},\n",
       " 'biology': {'topic': 0.3816200939306868,\n",
       "  'paper': 0.33347373404868963,\n",
       "  'speak': 0.2709193807064011,\n",
       "  'water': 0.26257443582637197,\n",
       "  'statement': 0.2341797342960526,\n",
       "  'fast': 0.20592601154720475}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculate similarities and produce for each source word a sorted (by descending similarity to the target word) a dictionary of target words and their respective similarities\n",
    "calculated_similarities = {key: {c: cos_similarity(word_vectors.get(key), word_vectors.get(c)) for c in target_words} for key in source_words}\n",
    "\n",
    "for source in calculated_similarities.keys():\n",
    "    calculated_similarities[source] = dict(sorted(calculated_similarities[source].items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "\n",
    "calculated_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_by_sim = {source_w: [key for key in sim_dict.keys()] for source_w, sim_dict in calculated_similarities.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>nose</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>run</th>\n",
       "      <th>biology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper</td>\n",
       "      <td>water</td>\n",
       "      <td>speak</td>\n",
       "      <td>fast</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fast</td>\n",
       "      <td>fast</td>\n",
       "      <td>statement</td>\n",
       "      <td>water</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>water</td>\n",
       "      <td>speak</td>\n",
       "      <td>fast</td>\n",
       "      <td>speak</td>\n",
       "      <td>speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>topic</td>\n",
       "      <td>paper</td>\n",
       "      <td>water</td>\n",
       "      <td>statement</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>speak</td>\n",
       "      <td>topic</td>\n",
       "      <td>topic</td>\n",
       "      <td>paper</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>statement</td>\n",
       "      <td>statement</td>\n",
       "      <td>paper</td>\n",
       "      <td>topic</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cat       nose pronounced        run    biology\n",
       "0      paper      water      speak       fast      topic\n",
       "1       fast       fast  statement      water      paper\n",
       "2      water      speak       fast      speak      speak\n",
       "3      topic      paper      water  statement      water\n",
       "4      speak      topic      topic      paper  statement\n",
       "5  statement  statement      paper      topic       fast"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So here is the overview of the target words sorted by relevancy to the respective source words (columns), according to the calculated cosine similarities.\n",
    "sim_df = pd.DataFrame(sorted_by_sim)\n",
    "sim_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assumed ranking of relevancy of target words to the source words (columns):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>nose</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>run</th>\n",
       "      <th>biology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ranking</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fast</td>\n",
       "      <td>speak</td>\n",
       "      <td>speak</td>\n",
       "      <td>fast</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>speak</td>\n",
       "      <td>water</td>\n",
       "      <td>fast</td>\n",
       "      <td>water</td>\n",
       "      <td>topic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>water</td>\n",
       "      <td>topic</td>\n",
       "      <td>topic</td>\n",
       "      <td>speak</td>\n",
       "      <td>fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>topic</td>\n",
       "      <td>statement</td>\n",
       "      <td>statement</td>\n",
       "      <td>topic</td>\n",
       "      <td>speak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paper</td>\n",
       "      <td>paper</td>\n",
       "      <td>water</td>\n",
       "      <td>statement</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>statement</td>\n",
       "      <td>fast</td>\n",
       "      <td>paper</td>\n",
       "      <td>paper</td>\n",
       "      <td>paper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               cat       nose pronounced        run    biology\n",
       "Ranking                                                       \n",
       "0             fast      speak      speak       fast      water\n",
       "1            speak      water       fast      water      topic\n",
       "2            water      topic      topic      speak       fast\n",
       "3            topic  statement  statement      topic      speak\n",
       "4            paper      paper      water  statement  statement\n",
       "5        statement       fast      paper      paper      paper"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To compare, here is the assumed ranking of relevancy of target words to the source words (columns):\n",
    "initial_assumption = pd.DataFrame(tables)\n",
    "initial_assumption.index.name = 'Ranking'\n",
    "print(\"Assumed ranking of relevancy of target words to the source words (columns):\")\n",
    "initial_assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that for some source words, the assumed ranking of relevancy of the targets is different from the calculated one, but for some source words like \"pronounced\" and \"run\" it is actually quite similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Calculating nearest neighbors (10 points).** For the 5 source words, retrieve the $k=10$ nearest neighbors using the word embedding model, namely the words with the highest similarities to the source word. Consider the following points: \n",
    "    \n",
    "- **Overall implementation (3/10 points):** your implemented function takes a source vector, a set of target vectors, and the $k$ parameter, and returns the $k$ nearest neighbors and their similarity scores. Implement nearest neighbor calculation by yourself and do NOT use the provided functionalities of any library.\n",
    "- **Similarity metrics (2/10 points):** execute the calculation of nearest neighbors according to <ins>two similarity metrics</ins> namely cosine and dot product.\n",
    "- **Efficiency (3/10 points):** your nearest neighbor functions should provide an *efficient* calculation of nearest neighbors. An inefficient way (which should be avoided!) would be looping over the set of vectors in the word embedding model, and one by one calculating the cosine/dot product similarity of the source vector to each of the target vectors. As a hint for an efficient way, consider that in `numpy` (and other libraries), calculating the dot product of a vector to a matrix is much faster than the dot products of the vector to each vector of the matrix.\n",
    "- **Reporting and observations (2/10 points):** report the results in tables, which enable comparing between the outputs of the two similarity metrics. Which similarity metric would you prefer? Report your observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the nearest neighbour calculation function for our context\n",
    "\n",
    "def nn(source_vector, target_vectors, k=10,  similarity_metric='cosine'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    source_vector: the word embedding vector of the word for which we want to find the k nearest neighbours according to the used word embedding model\n",
    "    target_vectors: the vectors of the words among which we are looking for the nearest neighbors in one np array as columns\n",
    "    k: the number of nearest neighbours we want to calculate\n",
    "    similarity_metric: \"cosine\" (default) or 'dot_product'\n",
    "    \"\"\"\n",
    "    if similarity_metric not in ['cosine', 'dot_product']:\n",
    "        raise ValueError(\"Invalid similarity metric. Use 'cosine' or 'dot_product'.\")\n",
    "\n",
    "    target_vectors_norm = np.zeros_like(target_vectors)\n",
    " \n",
    "    if similarity_metric == 'dot_product':\n",
    "        similarities = np.dot(source_vector, target_vectors)\n",
    "    elif similarity_metric == 'cosine':\n",
    "        #normalize the target vectors\n",
    "        target_vectors_norm = target_vectors/np.linalg.norm(target_vectors, axis = 0)\n",
    "            \n",
    "        similarities = np.dot(source_vector, target_vectors_norm/vec_norm(source_vector))\n",
    "\n",
    "    # Find indices of nearest neighbors\n",
    "    nearest_indices = np.argsort(similarities)[-2:-k-2:-1]\n",
    "\n",
    "    # Retrieve nearest neighbors and their similarity scores\n",
    "    nearest_neighbors = [(i, similarities[i]) for i in nearest_indices]\n",
    "\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create some dictionaries containing the vectors for souces and target words\n",
    "\n",
    "source = {w: word_vectors.get(w) for w in source_words}\n",
    "\n",
    "target_keys, target_vectors = [w for w in word_vectors.keys()], np.concatenate([v.reshape(300,1) for v in word_vectors.values()], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1917495)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the shape - we want the target vectors to be column vectors\n",
    "\n",
    "target_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the nearest neighbours according to cosine and dot similarity and store the results in the respective dictionaries.\n",
    "results_cosine = {}\n",
    "results_dot = {}\n",
    "\n",
    "for w, v in source.items():\n",
    "    results_cosine[w] = nn(v, target_vectors, similarity_metric='cosine')\n",
    "    results_dot[w] = nn(v, target_vectors, similarity_metric='dot_product')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A helper function for organizing the output dictionaries with the nn calculation into new dictionaries that are optimal for organizing into a dataframe\n",
    "def nn_for_words (source_words: List[str], target_words : List[str], results_sim: Dict):\n",
    "    result_dic = {}\n",
    "    \n",
    "    for source_w in source_words:\n",
    "        #df_cosine = pd.DataFrame(columns = target_words)\n",
    "        #\n",
    "        result_dic[source_w] = {}\n",
    "        for item in results_sim[source_w]:\n",
    "            result_dic[source_w][target_words[item[0]]] = item[1]\n",
    "\n",
    "    return result_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': {'cats': 0.7989761,\n",
       "  'dog': 0.7885449,\n",
       "  'kitten': 0.7550499,\n",
       "  'pet': 0.7330314,\n",
       "  'kitty': 0.688349,\n",
       "  'dogs': 0.6766945,\n",
       "  'puppy': 0.6289974,\n",
       "  'animal': 0.62568915,\n",
       "  'kittens': 0.6217527,\n",
       "  'pets': 0.60976696},\n",
       " 'nose': {'noses': 0.659544,\n",
       "  'ears': 0.6554991,\n",
       "  'ear': 0.63149667,\n",
       "  'mouth': 0.6278931,\n",
       "  'throat': 0.62265694,\n",
       "  'forehead': 0.5811095,\n",
       "  'cheeks': 0.57726973,\n",
       "  'eyes': 0.57719415,\n",
       "  'eye': 0.5748179,\n",
       "  'nostrils': 0.56571907},\n",
       " 'pronounced': {'spelled': 0.567124,\n",
       "  'pronounce': 0.5319733,\n",
       "  'pronunciation': 0.4920544,\n",
       "  'meaning': 0.4848815,\n",
       "  'noticeable': 0.4752436,\n",
       "  'known': 0.47403133,\n",
       "  'word': 0.4712189,\n",
       "  'slight': 0.46908784,\n",
       "  'spoken': 0.46723536,\n",
       "  'distinctly': 0.46124265},\n",
       " 'run': {'running': 0.83125305,\n",
       "  'runs': 0.8221512,\n",
       "  'ran': 0.76250213,\n",
       "  'going': 0.70628107,\n",
       "  'start': 0.69225335,\n",
       "  'go': 0.68862176,\n",
       "  'then': 0.6829677,\n",
       "  'put': 0.67353785,\n",
       "  'out': 0.6677391,\n",
       "  \"n't\": 0.6676805},\n",
       " 'biology': {'biochemistry': 0.8119485,\n",
       "  'physiology': 0.76672184,\n",
       "  'ecology': 0.7471285,\n",
       "  'science': 0.7343948,\n",
       "  'microbiology': 0.7322478,\n",
       "  'chemistry': 0.7183886,\n",
       "  'genetics': 0.7105685,\n",
       "  'sciences': 0.7009993,\n",
       "  'neuroscience': 0.6964635,\n",
       "  'immunology': 0.6847851}}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is the result of the nn calculation with cosine similarity (target words for each source word are already sorted acc. to descending similarity\n",
    "nn_cosine = nn_for_words(source_words, target_keys, results_cosine)\n",
    "nn_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat': {'dog': 43.716354,\n",
       "  'cats': 40.638523,\n",
       "  'pet': 39.949966,\n",
       "  'dogs': 37.221615,\n",
       "  'kitten': 34.478672,\n",
       "  'puppy': 33.31928,\n",
       "  'animal': 32.645176,\n",
       "  'pets': 32.630512,\n",
       "  'kitty': 31.441591,\n",
       "  'kittens': 31.008978},\n",
       " 'nose': {'ear': 34.47286,\n",
       "  'nasal': 33.86184,\n",
       "  'mouth': 33.74113,\n",
       "  'ears': 33.643345,\n",
       "  'throat': 33.36715,\n",
       "  'noses': 31.349174,\n",
       "  'eyes': 30.992413,\n",
       "  'neck': 30.938808,\n",
       "  'lips': 30.936796,\n",
       "  'forehead': 30.893946},\n",
       " 'pronounced': {'vowel': 19.999043,\n",
       "  'pronounce': 19.646309,\n",
       "  'pronunciation': 19.509834,\n",
       "  'word': 19.334837,\n",
       "  'spelled': 19.252337,\n",
       "  'syllable': 19.096249,\n",
       "  \"'\": 19.025759,\n",
       "  'was': 18.375347,\n",
       "  '\"': 18.327408,\n",
       "  'slight': 18.25709},\n",
       " 'run': {'running': 41.253323,\n",
       "  'runs': 37.896713,\n",
       "  'it': 33.980583,\n",
       "  'a': 33.814148,\n",
       "  'ran': 33.565823,\n",
       "  \"n't\": 33.480423,\n",
       "  'to': 33.258278,\n",
       "  'do': 33.06256,\n",
       "  'go': 33.01642,\n",
       "  'you': 32.841816},\n",
       " 'biology': {'biochemistry': 42.557316,\n",
       "  'science': 42.300243,\n",
       "  'physiology': 40.981117,\n",
       "  'sciences': 40.551636,\n",
       "  'genetics': 39.947605,\n",
       "  'chemistry': 39.58568,\n",
       "  'immunology': 38.718845,\n",
       "  'physics': 38.68036,\n",
       "  'molecular': 38.63976,\n",
       "  'ecology': 38.616726}}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is the result of the nn calculation with dot similarity (target words for each source word are already sorted acc. to descending similarity\n",
    "nn_dot = nn_for_words(source_words, target_keys, results_dot)\n",
    "nn_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 nearest neighbours according to cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>nose</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>run</th>\n",
       "      <th>biology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cats</td>\n",
       "      <td>noses</td>\n",
       "      <td>spelled</td>\n",
       "      <td>running</td>\n",
       "      <td>biochemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>ears</td>\n",
       "      <td>pronounce</td>\n",
       "      <td>runs</td>\n",
       "      <td>physiology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kitten</td>\n",
       "      <td>ear</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>ran</td>\n",
       "      <td>ecology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pet</td>\n",
       "      <td>mouth</td>\n",
       "      <td>meaning</td>\n",
       "      <td>going</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kitty</td>\n",
       "      <td>throat</td>\n",
       "      <td>noticeable</td>\n",
       "      <td>start</td>\n",
       "      <td>microbiology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dogs</td>\n",
       "      <td>forehead</td>\n",
       "      <td>known</td>\n",
       "      <td>go</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>puppy</td>\n",
       "      <td>cheeks</td>\n",
       "      <td>word</td>\n",
       "      <td>then</td>\n",
       "      <td>genetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>animal</td>\n",
       "      <td>eyes</td>\n",
       "      <td>slight</td>\n",
       "      <td>put</td>\n",
       "      <td>sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kittens</td>\n",
       "      <td>eye</td>\n",
       "      <td>spoken</td>\n",
       "      <td>out</td>\n",
       "      <td>neuroscience</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pets</td>\n",
       "      <td>nostrils</td>\n",
       "      <td>distinctly</td>\n",
       "      <td>n't</td>\n",
       "      <td>immunology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat      nose     pronounced      run       biology\n",
       "0     cats     noses        spelled  running  biochemistry\n",
       "1      dog      ears      pronounce     runs    physiology\n",
       "2   kitten       ear  pronunciation      ran       ecology\n",
       "3      pet     mouth        meaning    going       science\n",
       "4    kitty    throat     noticeable    start  microbiology\n",
       "5     dogs  forehead          known       go     chemistry\n",
       "6    puppy    cheeks           word     then      genetics\n",
       "7   animal      eyes         slight      put      sciences\n",
       "8  kittens       eye         spoken      out  neuroscience\n",
       "9     pets  nostrils     distinctly      n't    immunology"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, here we have the comparison of the found 10 nearest neighbours of the chosen source words\n",
    "sorted_by_sim_cosine = {source_w: [key for key in sim_dict.keys()] for source_w, sim_dict in nn_cosine.items()}\n",
    "df_cosine = pd.DataFrame(sorted_by_sim_cosine)\n",
    "df_cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 nearest neighbours according to dot similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>nose</th>\n",
       "      <th>pronounced</th>\n",
       "      <th>run</th>\n",
       "      <th>biology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog</td>\n",
       "      <td>ear</td>\n",
       "      <td>vowel</td>\n",
       "      <td>running</td>\n",
       "      <td>biochemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cats</td>\n",
       "      <td>nasal</td>\n",
       "      <td>pronounce</td>\n",
       "      <td>runs</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pet</td>\n",
       "      <td>mouth</td>\n",
       "      <td>pronunciation</td>\n",
       "      <td>it</td>\n",
       "      <td>physiology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dogs</td>\n",
       "      <td>ears</td>\n",
       "      <td>word</td>\n",
       "      <td>a</td>\n",
       "      <td>sciences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kitten</td>\n",
       "      <td>throat</td>\n",
       "      <td>spelled</td>\n",
       "      <td>ran</td>\n",
       "      <td>genetics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>puppy</td>\n",
       "      <td>noses</td>\n",
       "      <td>syllable</td>\n",
       "      <td>n't</td>\n",
       "      <td>chemistry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>animal</td>\n",
       "      <td>eyes</td>\n",
       "      <td>'</td>\n",
       "      <td>to</td>\n",
       "      <td>immunology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pets</td>\n",
       "      <td>neck</td>\n",
       "      <td>was</td>\n",
       "      <td>do</td>\n",
       "      <td>physics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kitty</td>\n",
       "      <td>lips</td>\n",
       "      <td>\"</td>\n",
       "      <td>go</td>\n",
       "      <td>molecular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>kittens</td>\n",
       "      <td>forehead</td>\n",
       "      <td>slight</td>\n",
       "      <td>you</td>\n",
       "      <td>ecology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cat      nose     pronounced      run       biology\n",
       "0      dog       ear          vowel  running  biochemistry\n",
       "1     cats     nasal      pronounce     runs       science\n",
       "2      pet     mouth  pronunciation       it    physiology\n",
       "3     dogs      ears           word        a      sciences\n",
       "4   kitten    throat        spelled      ran      genetics\n",
       "5    puppy     noses       syllable      n't     chemistry\n",
       "6   animal      eyes              '       to    immunology\n",
       "7     pets      neck            was       do       physics\n",
       "8    kitty      lips              \"       go     molecular\n",
       "9  kittens  forehead         slight      you       ecology"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_by_sim_dot = {source_w: [key for key in sim_dict.keys()] for source_w, sim_dict in nn_dot.items()}\n",
    "df_dot = pd.DataFrame(sorted_by_sim_dot)\n",
    "df_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskB\"></a><h2 style=\"color:rgb(0,120,170)\">Task B: Document Classification with WE (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This task follows the same instruction for document classification as provided in Assignment 1. You are indeed free to reuse any part of your code in Assignment 1 for this task. In Assignment 1, the representation of each document was created using a bag of words representation followed by dimensionality reduction. In this task, the document representations are created from the pre-trained word embeddings.\n",
    "\n",
    "**Map word embeddings to dictionary words (5 points).** For every word in the dictionary (as discussed and created in Assignment 1), fetch the corresponding word embedding from the pre-trained model. If no embedding is found, initialize the corresponding word embedding randomly.\n",
    "\n",
    "**Document embedding as the average of word embeddings (5 points).** Using the word embeddings, the representation of each document is defined as the *mean of the vectors of each document's words*. In particular, given the document $d$, consisting of words $\\left[ v_1, v_2, ..., v_{|d|} \\right]$, the document representation $\\mathbf{e}_d$ is defined as:\n",
    "\n",
    "$\\mathbf{e}_d = \\frac{1}{|d|}\\sum_{i=1}^{|d|}{\\mathbf{e}_{v_i}}$\n",
    "\n",
    "where $\\mathbf{e}_{v}$ is the vector of the word $v$, and $|d|$ is the length of the document.\n",
    "\n",
    "**Classification and evaluation (5 points)** Using these new document representations, apply <ins>three classification algorithms</ins> and report the evaluation results (based on accuracy metric) on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START CREATING DICTIONARY-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate one dictionary containing all tokens that appear accross all texts \n",
    "# and count the number of their overall occurences\n",
    "word_dict = dict(Counter(list(itertools.chain(*tokens_per_text_train))))\n",
    "# sort the dictionary by the number of word occurences\n",
    "word_dict = dict(sorted(word_dict.items(), key=lambda x:x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the size of the dictionary by applying a threshold value to the number of occurences\n",
    "threshold = 100 \n",
    "smaller_word_dict = {k:v for (k,v) in word_dict.items() if v > threshold}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23865\n",
      "951\n"
     ]
    }
   ],
   "source": [
    "print(len(word_dict))\n",
    "print(len(smaller_word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END CREATING DICTIONARY-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smaller_word_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[205], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[43msmaller_word_dict\u001b[49m:\n\u001b[0;32m      3\u001b[0m     wv\u001b[38;5;241m=\u001b[39m word_vectors\u001b[38;5;241m.\u001b[39mget(w)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'smaller_word_dict' is not defined"
     ]
    }
   ],
   "source": [
    "word_vectors = {}\n",
    "for w in smaller_word_dict:\n",
    "    wv= word_vectors.get(w)\n",
    "    if wv is not None:\n",
    "        word_vectors[w] = wv\n",
    "    else:\n",
    "        word_vectors[w] = np.random.rand(300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"section-taskC\"></a><h2 style=\"color:rgb(0,120,170)\">Task C: Classification with sent2vec Document Embeddings (2 extra point)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sent2vec [1] suggests another unsupervised approach to creating document embeddings from the underlying word embeddings. First, using the provided code in the paper, train a sendtvec model on the training set to create document embeddings. Then, repeat Task B while using the document embeddings provided by sent2vec. Similar to Task 2, conduct the classification experiments and report evaluation results.\n",
    "\n",
    "[1] M. Pagliardini, P. Gupta, and M. Jaggi. Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features. In Proceedings of the conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2018.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
